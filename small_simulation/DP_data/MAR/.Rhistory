# Sample V_k from k = 1,...,K-1
for (k in 1:(K_true - 1)) {
V[k] = rbeta(1, 1, gamma_true) # Vk ~ Beta(1, gamma)
Beta_true[k] = V[k]*prod
prod = prod*(1-V[k])
}
Beta_true[K_true] = 1-sum(Beta_true[1:(K_true-1)])
# Pi_i ~ DP(alpha_true, Beta_true) for i = 1,...,N
Pi_true = matrix(NA, N, K_true)
u = matrix(NA, N, K_true)
u[,K_true] = 1
prod = array(1, N) # product of (1-u_il) for l<k
# sample u_ik ~ Beta distribution
# then, obtain Pi from u
for (k in 1:(K_true - 1)) {
u[,k] = rbeta(N, alpha_true*Beta_true[k], alpha_true*(1-sum(Beta_true[1:k])))
Pi_true[,k] = u[,k]*prod
prod = prod*(1-u[,k])
}
Pi_true[,K_true] = 1-rowSums(Pi_true[,1:(K_true-1)])
# Check the mean of Pi from DP
cat('Beta:', Beta_true,'\n')
cat('Pi:', apply(Pi_true, MARGIN = 2, FUN = mean))
# Sample cluster assignment z_ij ~ Cat(Pi_i)
# Sample observation x_ij|z_ij=k ~ Cat(Phi_jk)
z_true = matrix(NA, nrow = N, ncol = p) # true cluster assignment
X = matrix(NA, nrow = N, ncol = p) # data matrix
for (i in 1:N) {
z_true[i,] = sample(1:K_true, p, replace=TRUE, prob=Pi_true[i,])
for (j in 1:p) {
z = z_true[i,j]
prob = Phi[[j]][z,]
X[i,j] = sample(1:length(prob), 1, replace = TRUE, prob = prob)
}
}
# make MAR
X_miss = X
for (i in 1:N) {
x3 = X_miss[i,3]
if (x3==1) {
if (runif(1)<=0.05) X_miss[i,1] = NA
if (runif(1)<=0.55) X_miss[i,2] = NA
}else if (x3==2) {
if (runif(1)<=0.55) X_miss[i,1] = NA
if (runif(1)<=0.1) X_miss[i,2] = NA
}else{
if (runif(1)<=0.1) X_miss[i,1] = NA
if (runif(1)<=0.5) X_miss[i,2] = NA
}
}
R = is.na(X_miss)
print(apply(R, MARGIN = 2, FUN = mean))
# 3rd variable
var = 3
barplot(table(X[,var])/N, main = 'overall')
for (z in 1:K_true) {
barplot(table(X[z_true[,var]==z,var])/sum(table(X[z_true[,var]==z,var])),
main = paste('variable:',var,'/', 'cluster:',z), ylim = c(0,1))
}
# Compare true marginal distribution with posterior samples
for (var in 1:p) {
true_pmf = table(X[,var])/N
observed_pmf = table(X_miss[R[,var]!=1,var])
observed_pmf = observed_pmf/sum(observed_pmf)
df = rbind(observed_pmf, true_pmf)
barplot(df, xlab = 'Category', beside = TRUE,
legend = TRUE, main = paste('Observed pmf vs True pmf: j =', var), ylim = c(0,1))
}
knitr::opts_chunk$set(echo = TRUE, fig.pos = 'h', fig.align = 'center')
knitr::opts_chunk$set(fig.cap = "",  fig.path = "Plot")
library(knitr)
library(dplyr)
library(arm)
library(pROC)
library(tidyverse)
library(MASS)
library(tigerstats)
library(leaps)
library(car)
library(rms)
require(caret)
require(e1071)
library(lme4)
library(lattice)
library(broom)
library(boot)
library(ggplot2)
library(cobalt)
require(tidyverse)
require(rstanarm)
require(magrittr)
require(rstan)
require(MCMCpack)
library(abind)
library(matrixStats)
library(truncnorm)
library(mvtnorm)
library(MCMCpack)
library(mnormt)
library(coda)
set.seed(1)
# parameter setup
N = 2000 # number of surveyed subjects
K_true = 3 # number of true clusteres
p = 3 # number of survey questions
d1 = 2 # number of levels of 1st variable
d2 = 3 # number of levels of 2st variable
d3 = 3 # number of levels of 3st variable
gamma_true = 2
alpha_true = 1
level = c(d1, d2, d3)
# place holder
Phi_1 = matrix(NA, nrow = K_true, ncol = d1) # multinomial parameter for 1st variable
Phi_2 = matrix(NA, nrow = K_true, ncol = d2) # multinomial parameter for 2nd variable
Phi_3 = matrix(NA, nrow = K_true, ncol = d3) # multinomial parameter for 3rd variable
# Phi parameter for different clusters and different variables
# size #clusterx#levels
# 1st variable (d1 = 2)
Phi_1[,1] = c(0.1, 0.5, 0.9)
Phi_1[,2] = 1 - Phi_1[,1]
# 2nd variable (d2 = 3)
Phi_2[,1] = c(0.1, 0.1, 0.8)
Phi_2[,2] = c(0.1, 0.8, 0.1)
Phi_2[,3] = 1 - Phi_2[,1] - Phi_2[,2]
# 3rd variable (d3 = 3)
Phi_3[,1] = c(0.1, 0.1, 0.8)
Phi_3[,2] = c(0.8, 0.1, 0.1)
Phi_3[,3] = 1 - Phi_3[,1] - Phi_3[,2]
Phi = list(Phi_1, Phi_2, Phi_3)
# Beta ~ GEM(gamma_true)
Beta_true = array(NA, dim = K_true)
V = array(NA, K_true)
V[K_true] = 1
prod = 1 # product of (1-V_l) for l<k
# Sample V_k from k = 1,...,K-1
for (k in 1:(K_true - 1)) {
V[k] = rbeta(1, 1, gamma_true) # Vk ~ Beta(1, gamma)
Beta_true[k] = V[k]*prod
prod = prod*(1-V[k])
}
Beta_true[K_true] = 1-sum(Beta_true[1:(K_true-1)])
# Pi_i ~ DP(alpha_true, Beta_true) for i = 1,...,N
Pi_true = matrix(NA, N, K_true)
u = matrix(NA, N, K_true)
u[,K_true] = 1
prod = array(1, N) # product of (1-u_il) for l<k
# sample u_ik ~ Beta distribution
# then, obtain Pi from u
for (k in 1:(K_true - 1)) {
u[,k] = rbeta(N, alpha_true*Beta_true[k], alpha_true*(1-sum(Beta_true[1:k])))
Pi_true[,k] = u[,k]*prod
prod = prod*(1-u[,k])
}
Pi_true[,K_true] = 1-rowSums(Pi_true[,1:(K_true-1)])
# Check the mean of Pi from DP
cat('Beta:', Beta_true,'\n')
cat('Pi:', apply(Pi_true, MARGIN = 2, FUN = mean))
# Sample cluster assignment z_ij ~ Cat(Pi_i)
# Sample observation x_ij|z_ij=k ~ Cat(Phi_jk)
z_true = matrix(NA, nrow = N, ncol = p) # true cluster assignment
X = matrix(NA, nrow = N, ncol = p) # data matrix
for (i in 1:N) {
z_true[i,] = sample(1:K_true, p, replace=TRUE, prob=Pi_true[i,])
for (j in 1:p) {
z = z_true[i,j]
prob = Phi[[j]][z,]
X[i,j] = sample(1:length(prob), 1, replace = TRUE, prob = prob)
}
}
# make MAR
X_miss = X
for (i in 1:N) {
x3 = X_miss[i,3]
if (x3==1) {
if (runif(1)<=0.05) X_miss[i,1] = NA
if (runif(1)<=0.55) X_miss[i,2] = NA
}else if (x3==2) {
if (runif(1)<=0.5) X_miss[i,1] = NA
if (runif(1)<=0.1) X_miss[i,2] = NA
}else{
if (runif(1)<=0.1) X_miss[i,1] = NA
if (runif(1)<=0.5) X_miss[i,2] = NA
}
}
R = is.na(X_miss)
print(apply(R, MARGIN = 2, FUN = mean))
# 3rd variable
var = 3
barplot(table(X[,var])/N, main = 'overall')
for (z in 1:K_true) {
barplot(table(X[z_true[,var]==z,var])/sum(table(X[z_true[,var]==z,var])),
main = paste('variable:',var,'/', 'cluster:',z), ylim = c(0,1))
}
# Compare true marginal distribution with posterior samples
for (var in 1:p) {
true_pmf = table(X[,var])/N
observed_pmf = table(X_miss[R[,var]!=1,var])
observed_pmf = observed_pmf/sum(observed_pmf)
df = rbind(observed_pmf, true_pmf)
barplot(df, xlab = 'Category', beside = TRUE,
legend = TRUE, main = paste('Observed pmf vs True pmf: j =', var), ylim = c(0,1))
}
knitr::opts_chunk$set(echo = TRUE, fig.pos = 'h', fig.align = 'center')
knitr::opts_chunk$set(fig.cap = "",  fig.path = "Plot")
library(knitr)
library(dplyr)
library(arm)
library(pROC)
library(tidyverse)
library(MASS)
library(tigerstats)
library(leaps)
library(car)
library(rms)
require(caret)
require(e1071)
library(lme4)
library(lattice)
library(broom)
library(boot)
library(ggplot2)
library(cobalt)
require(tidyverse)
require(rstanarm)
require(magrittr)
require(rstan)
require(MCMCpack)
library(abind)
library(matrixStats)
library(truncnorm)
library(mvtnorm)
library(MCMCpack)
library(mnormt)
library(coda)
set.seed(1)
# parameter setup
N = 2000 # number of surveyed subjects
K_true = 3 # number of true clusteres
p = 3 # number of survey questions
d1 = 2 # number of levels of 1st variable
d2 = 3 # number of levels of 2st variable
d3 = 3 # number of levels of 3st variable
gamma_true = 2
alpha_true = 1
level = c(d1, d2, d3)
# place holder
Phi_1 = matrix(NA, nrow = K_true, ncol = d1) # multinomial parameter for 1st variable
Phi_2 = matrix(NA, nrow = K_true, ncol = d2) # multinomial parameter for 2nd variable
Phi_3 = matrix(NA, nrow = K_true, ncol = d3) # multinomial parameter for 3rd variable
# Phi parameter for different clusters and different variables
# size #clusterx#levels
# 1st variable (d1 = 2)
Phi_1[,1] = c(0.1, 0.5, 0.9)
Phi_1[,2] = 1 - Phi_1[,1]
# 2nd variable (d2 = 3)
Phi_2[,1] = c(0.1, 0.1, 0.8)
Phi_2[,2] = c(0.1, 0.8, 0.1)
Phi_2[,3] = 1 - Phi_2[,1] - Phi_2[,2]
# 3rd variable (d3 = 3)
Phi_3[,1] = c(0.1, 0.1, 0.8)
Phi_3[,2] = c(0.8, 0.1, 0.1)
Phi_3[,3] = 1 - Phi_3[,1] - Phi_3[,2]
Phi = list(Phi_1, Phi_2, Phi_3)
# Beta ~ GEM(gamma_true)
Beta_true = array(NA, dim = K_true)
V = array(NA, K_true)
V[K_true] = 1
prod = 1 # product of (1-V_l) for l<k
# Sample V_k from k = 1,...,K-1
for (k in 1:(K_true - 1)) {
V[k] = rbeta(1, 1, gamma_true) # Vk ~ Beta(1, gamma)
Beta_true[k] = V[k]*prod
prod = prod*(1-V[k])
}
Beta_true[K_true] = 1-sum(Beta_true[1:(K_true-1)])
# Sample cluster assignment z_i ~ Cat(Beta)
# Sample observation x_ij|z_i=k ~ Cat(Phi_jk)
z_true = sample(1:K_true, N, replace = TRUE, prob = Beta_true) # true cluster assignment
X = matrix(NA, nrow = N, ncol = p) # data matrix
for (i in 1:N) {
z = z_true[i]
for (j in 1:p) {
prob = Phi[[j]][z,]
X[i,j] = sample(1:length(prob), 1, replace = TRUE, prob = prob)
}
}
# make MAR
X_miss = X
for (i in 1:N) {
x3 = X_miss[i,3]
if (x3==1) {
if (runif(1)<=0.5) X_miss[i,1] = NA
if (runif(1)<=0.05) X_miss[i,2] = NA
}else if (x3==2) {
if (runif(1)<=0.1) X_miss[i,1] = NA
if (runif(1)<=0.45) X_miss[i,2] = NA
}else{
if (runif(1)<=0.5) X_miss[i,1] = NA
if (runif(1)<=0.3) X_miss[i,2] = NA
}
}
R = is.na(X_miss)
print(apply(R, MARGIN = 2, FUN = mean))
barplot(table(z_true)/N, main = 'cluster assignment: z', ylim = c(0,1))
# 1st variable
var = 1
barplot(table(X[,var])/N, main = 'overall')
for (z in 1:K_true) {
barplot(table(X[z_true==z,var])/sum(table(X[z_true==z,var])),
main = paste('variable:',var,'/', 'cluster:',z), ylim = c(0,1))
}
# 2nd variable
var = 2
barplot(table(X[,var])/N, main = 'overall')
for (z in 1:K_true) {
barplot(table(X[z_true==z,var])/sum(table(X[z_true==z,var])),
main = paste('variable:',var,'/', 'cluster:',z), ylim = c(0,1))
}
# 3rd variable
var = 3
barplot(table(X[,var])/N, main = 'overall')
for (z in 1:K_true) {
barplot(table(X[z_true==z,var])/sum(table(X[z_true==z,var])),
main = paste('variable:',var,'/', 'cluster:',z), ylim = c(0,1))
}
# Compare true marginal distribution with posterior samples
for (var in 1:p) {
true_pmf = table(X[,var])/N
observed_pmf = table(X_miss[R[,var]!=1,var])
observed_pmf = observed_pmf/sum(observed_pmf)
df = rbind(observed_pmf, true_pmf)
barplot(df, xlab = 'Category', beside = TRUE,
legend = TRUE, main = paste('Observed pmf vs True pmf: j =', var), ylim = c(0,1))
}
knitr::opts_chunk$set(echo = TRUE, fig.pos = 'h', fig.align = 'center')
knitr::opts_chunk$set(fig.cap = "",  fig.path = "Plot")
library(knitr)
library(dplyr)
library(arm)
library(pROC)
library(tidyverse)
library(MASS)
library(tigerstats)
library(leaps)
library(car)
library(rms)
require(caret)
require(e1071)
library(lme4)
library(lattice)
library(broom)
library(boot)
library(ggplot2)
library(cobalt)
require(tidyverse)
require(rstanarm)
require(magrittr)
require(rstan)
require(MCMCpack)
library(abind)
library(matrixStats)
library(truncnorm)
library(mvtnorm)
library(MCMCpack)
library(mnormt)
library(coda)
set.seed(1)
# parameter setup
N = 2000 # number of surveyed subjects
K_true = 3 # number of true clusteres
p = 3 # number of survey questions
d1 = 2 # number of levels of 1st variable
d2 = 3 # number of levels of 2st variable
d3 = 3 # number of levels of 3st variable
gamma_true = 2
alpha_true = 1
level = c(d1, d2, d3)
# place holder
Phi_1 = matrix(NA, nrow = K_true, ncol = d1) # multinomial parameter for 1st variable
Phi_2 = matrix(NA, nrow = K_true, ncol = d2) # multinomial parameter for 2nd variable
Phi_3 = matrix(NA, nrow = K_true, ncol = d3) # multinomial parameter for 3rd variable
# Phi parameter for different clusters and different variables
# size #clusterx#levels
# 1st variable (d1 = 2)
Phi_1[,1] = c(0.1, 0.5, 0.9)
Phi_1[,2] = 1 - Phi_1[,1]
# 2nd variable (d2 = 3)
Phi_2[,1] = c(0.1, 0.1, 0.8)
Phi_2[,2] = c(0.1, 0.8, 0.1)
Phi_2[,3] = 1 - Phi_2[,1] - Phi_2[,2]
# 3rd variable (d3 = 3)
Phi_3[,1] = c(0.1, 0.1, 0.8)
Phi_3[,2] = c(0.8, 0.1, 0.1)
Phi_3[,3] = 1 - Phi_3[,1] - Phi_3[,2]
Phi = list(Phi_1, Phi_2, Phi_3)
# Beta ~ GEM(gamma_true)
Beta_true = array(NA, dim = K_true)
V = array(NA, K_true)
V[K_true] = 1
prod = 1 # product of (1-V_l) for l<k
# Sample V_k from k = 1,...,K-1
for (k in 1:(K_true - 1)) {
V[k] = rbeta(1, 1, gamma_true) # Vk ~ Beta(1, gamma)
Beta_true[k] = V[k]*prod
prod = prod*(1-V[k])
}
Beta_true[K_true] = 1-sum(Beta_true[1:(K_true-1)])
# Sample cluster assignment z_i ~ Cat(Beta)
# Sample observation x_ij|z_i=k ~ Cat(Phi_jk)
z_true = sample(1:K_true, N, replace = TRUE, prob = Beta_true) # true cluster assignment
X = matrix(NA, nrow = N, ncol = p) # data matrix
for (i in 1:N) {
z = z_true[i]
for (j in 1:p) {
prob = Phi[[j]][z,]
X[i,j] = sample(1:length(prob), 1, replace = TRUE, prob = prob)
}
}
# make MAR
X_miss = X
for (i in 1:N) {
x3 = X_miss[i,3]
if (x3==1) {
if (runif(1)<=0.5) X_miss[i,1] = NA
if (runif(1)<=0.05) X_miss[i,2] = NA
}else if (x3==2) {
if (runif(1)<=0.1) X_miss[i,1] = NA
if (runif(1)<=0.45) X_miss[i,2] = NA
}else{
if (runif(1)<=0.5) X_miss[i,1] = NA
if (runif(1)<=0.3) X_miss[i,2] = NA
}
}
R = is.na(X_miss)
print(apply(R, MARGIN = 2, FUN = mean))
barplot(table(z_true)/N, main = 'cluster assignment: z', ylim = c(0,1))
# 1st variable
var = 1
barplot(table(X[,var])/N, main = 'overall')
for (z in 1:K_true) {
barplot(table(X[z_true==z,var])/sum(table(X[z_true==z,var])),
main = paste('variable:',var,'/', 'cluster:',z), ylim = c(0,1))
}
# 2nd variable
var = 2
barplot(table(X[,var])/N, main = 'overall')
for (z in 1:K_true) {
barplot(table(X[z_true==z,var])/sum(table(X[z_true==z,var])),
main = paste('variable:',var,'/', 'cluster:',z), ylim = c(0,1))
}
# 3rd variable
var = 3
barplot(table(X[,var])/N, main = 'overall')
for (z in 1:K_true) {
barplot(table(X[z_true==z,var])/sum(table(X[z_true==z,var])),
main = paste('variable:',var,'/', 'cluster:',z), ylim = c(0,1))
}
set.seed(1)
# MCMC variables
Mon = 20000
burnin = 10000
thining = 50
n_samples = (Mon-burnin)/thining
N = dim(X)[1]
p = dim(X)[2]
# storage
X_POST = array(NA, c(N, p, n_samples))
# Impute by using marginal probability
for (i in 1:n_samples) {
X_POST[,,i] <- X_miss
for (col in 1:ncol(X)) {
missing_ind <- R[,col]
X_POST[missing_ind, col, i] <- sample(1:level[col], size = sum(missing_ind),
table(X_miss[R[,col]!=1,col]), replace = TRUE)
}
}
# Compare true marginal distribution with posterior samples
for (var in 1:p) {
true_pmf = table(X[,var])/N
imputed_pmf = table(X_POST[,var,])
imputed_pmf = imputed_pmf/sum(imputed_pmf)
df = rbind(imputed_pmf, true_pmf)
barplot(df, xlab = 'Category', beside = TRUE,
legend = TRUE, main = paste('Posterior Predictive Distribution Assessment: j =', var))
}
# Evaluate joint pmf: 1,2
true_pmf = table(X[,1], X[,2])
true_pmf = c(true_pmf/sum(true_pmf))
imputed_pmf = table(X_POST[,1,], X_POST[,2,])
imputed_pmf = c(imputed_pmf/sum(imputed_pmf))
df = rbind(imputed_pmf, true_pmf)
barplot(df, xlab = 'Category', beside = TRUE,
legend = TRUE, main = paste('Posterior Predictive Distribution Assessment: joint pmf 1,2'),
ylim = c(0,0.5))
# Evaluate joint pmf: 2,3
true_pmf = table(X[,2], X[,3])
true_pmf = c(true_pmf/sum(true_pmf))
imputed_pmf = table(X_POST[,2,], X_POST[,3,])
imputed_pmf = c(imputed_pmf/sum(imputed_pmf))
df = rbind(imputed_pmf, true_pmf)
barplot(df, xlab = 'Category', beside = TRUE,
legend = TRUE, main = paste('Posterior Predictive Distribution Assessment: joint pmf 2,3'),
ylim = c(0,0.5))
# Evaluate joint pmf: 1,3
true_pmf = table(X[,1], X[,3])
true_pmf = c(true_pmf/sum(true_pmf))
imputed_pmf = table(X_POST[,1,], X_POST[,3,])
imputed_pmf = c(imputed_pmf/sum(imputed_pmf))
df = rbind(imputed_pmf, true_pmf)
barplot(df, xlab = 'Category', beside = TRUE,
legend = TRUE, main = paste('Posterior Predictive Distribution Assessment: joint pmf 1,3'),
ylim = c(0,0.5))
