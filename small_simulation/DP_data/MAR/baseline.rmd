
---
title: "Baseline for MI - Simulation example with 30% MCAR"
output:
  pdf_document: default
  html_document:
    highlight: pygments
    theme: spacelab
---

```{r setup, echo =FALSE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.pos = 'h', fig.align = 'center')
knitr::opts_chunk$set(fig.cap = "",  fig.path = "Plot")
library(knitr)
library(dplyr)
library(arm)
library(pROC)
library(tidyverse)
library(MASS)
library(tigerstats)
library(leaps)
library(car)
library(rms)
require(caret)
require(e1071)
library(lme4) 
library(lattice)
library(broom)
library(boot)
library(ggplot2)
library(cobalt)
require(tidyverse)
require(rstanarm)
require(magrittr)
require(rstan)
require(MCMCpack)
library(abind)
library(matrixStats)
library(truncnorm)
library(mvtnorm)
library(MCMCpack)
library(mnormt)
library(coda)
```

* * *
### Generate small dataset

Settings 
- N = 2000 surveyed subjects
- K = 3 clusters
- p = 3 questions/variables
- d1 = 2 choices while d2 = d3 = 3 choices
- $\gamma$ = 2 and $\alpha_0$ = 1 for DP generative process

Generative process
- $\beta|\gamma \sim GEM(\gamma)$
- $\pi_i|\alpha_0 \sim DP(\alpha_0, \beta)$ for i = 1,...,N
- $z_{ij}|\pi_i \sim Cat(\pi_i)$ for i = 1,...,N and j = 1,...,p
- $x_{ij}|z_{ij}=k \sim Cat(\Phi^{(j)}_k)$ for i = 1,...,N and j=1,...,p

```{r}
set.seed(1)
# parameter setup
N = 2000 # number of surveyed subjects
K_true = 3 # number of true clusteres
p = 3 # number of survey questions
d1 = 2 # number of levels of 1st variable
d2 = 3 # number of levels of 2st variable
d3 = 3 # number of levels of 3st variable
gamma_true = 2
alpha_true = 1
level = c(d1, d2, d3)

# place holder
Phi_1 = matrix(NA, nrow = K_true, ncol = d1) # multinomial parameter for 1st variable
Phi_2 = matrix(NA, nrow = K_true, ncol = d2) # multinomial parameter for 2nd variable
Phi_3 = matrix(NA, nrow = K_true, ncol = d3) # multinomial parameter for 3rd variable

```

```{r}
# Phi parameter for different clusters and different variables
# size #clusterx#levels

# 1st variable (d1 = 2)
Phi_1[,1] = c(0.1, 0.5, 0.9)
Phi_1[,2] = 1 - Phi_1[,1]

# 2nd variable (d2 = 3)
Phi_2[,1] = c(0.1, 0.1, 0.8)
Phi_2[,2] = c(0.1, 0.8, 0.1)
Phi_2[,3] = 1 - Phi_2[,1] - Phi_2[,2]

# 3rd variable (d3 = 3)
Phi_3[,1] = c(0.1, 0.1, 0.8)
Phi_3[,2] = c(0.8, 0.1, 0.1)
Phi_3[,3] = 1 - Phi_3[,1] - Phi_3[,2]

Phi = list(Phi_1, Phi_2, Phi_3)
```

```{r}
# Beta ~ GEM(gamma_true)
Beta_true = array(NA, dim = K_true)
V = array(NA, K_true)
V[K_true] = 1
prod = 1 # product of (1-V_l) for l<k
# Sample V_k from k = 1,...,K-1
for (k in 1:(K_true - 1)) {
  V[k] = rbeta(1, 1, gamma_true) # Vk ~ Beta(1, gamma)
  Beta_true[k] = V[k]*prod
  prod = prod*(1-V[k])
}
Beta_true[K_true] = 1-sum(Beta_true[1:(K_true-1)])
```

```{r}
# Sample cluster assignment z_i ~ Cat(Beta)
# Sample observation x_ij|z_i=k ~ Cat(Phi_jk)
z_true = sample(1:K_true, N, replace = TRUE, prob = Beta_true) # true cluster assignment
X = matrix(NA, nrow = N, ncol = p) # data matrix
for (i in 1:N) {
  z = z_true[i]
  for (j in 1:p) {
    prob = Phi[[j]][z,]
    X[i,j] = sample(1:length(prob), 1, replace = TRUE, prob = prob)
  }
}
```

```{r}
# make MAR
X_miss = X
for (i in 1:N) {
  x3 = X_miss[i,3]
  if (x3==1) {
    if (runif(1)<=0.5) X_miss[i,1] = NA
    if (runif(1)<=0.05) X_miss[i,2] = NA
  }else if (x3==2) {
    if (runif(1)<=0.1) X_miss[i,1] = NA
    if (runif(1)<=0.45) X_miss[i,2] = NA
  }else{
    if (runif(1)<=0.5) X_miss[i,1] = NA
    if (runif(1)<=0.3) X_miss[i,2] = NA
  }
}
R = is.na(X_miss)
print(apply(R, MARGIN = 2, FUN = mean))
```

### Visualization

```{r}
barplot(table(z_true)/N, main = 'cluster assignment: z', ylim = c(0,1))
```

```{r}
# 1st variable
var = 1
barplot(table(X[,var])/N, main = 'overall')
for (z in 1:K_true) {
  barplot(table(X[z_true==z,var])/sum(table(X[z_true==z,var])), 
        main = paste('variable:',var,'/', 'cluster:',z), ylim = c(0,1))
}
```

```{r}
# 2nd variable
var = 2
barplot(table(X[,var])/N, main = 'overall')
for (z in 1:K_true) {
  barplot(table(X[z_true==z,var])/sum(table(X[z_true==z,var])), 
        main = paste('variable:',var,'/', 'cluster:',z), ylim = c(0,1))
}
```

```{r}
# 3rd variable
var = 3
barplot(table(X[,var])/N, main = 'overall')
for (z in 1:K_true) {
  barplot(table(X[z_true==z,var])/sum(table(X[z_true==z,var])), 
        main = paste('variable:',var,'/', 'cluster:',z), ylim = c(0,1))
}
```

### Mixed-Membership Model (MMM) MCMC

```{r}
set.seed(1)
# MCMC variables
Mon = 20000
burnin = 10000
thining = 50
n_samples = (Mon-burnin)/thining

N = dim(X)[1]
p = dim(X)[2]

# storage
X_POST = array(NA, c(N, p, n_samples))

# Impute by using marginal probability
for (i in 1:n_samples) {
  X_POST[,,i] <- X_miss
  for (col in 1:ncol(X)) {
    missing_ind <- R[,col]
    X_POST[missing_ind, col, i] <- sample(1:level[col], size = sum(missing_ind),
                                       table(X_miss[R[,col]!=1,col]), replace = TRUE)
  }
}
```



### Posterior Diagnostics

```{r}
# Compare true marginal distribution with posterior samples
for (var in 1:p) {
  true_pmf = table(X[,var])/N
  imputed_pmf = table(X_POST[,var,])
  imputed_pmf = imputed_pmf/sum(imputed_pmf)
  df = rbind(imputed_pmf, true_pmf)
  barplot(df, xlab = 'Category', beside = TRUE, 
        legend = TRUE, main = paste('Posterior Predictive Distribution Assessment: j =', var))
}
```

```{r}
# Evaluate joint pmf: 1,2
true_pmf = table(X[,1], X[,2])
true_pmf = c(true_pmf/sum(true_pmf))
imputed_pmf = table(X_POST[,1,], X_POST[,2,])
imputed_pmf = c(imputed_pmf/sum(imputed_pmf))
df = rbind(imputed_pmf, true_pmf)
barplot(df, xlab = 'Category', beside = TRUE, 
      legend = TRUE, main = paste('Posterior Predictive Distribution Assessment: joint pmf 1,2'),
      ylim = c(0,0.5))
```

```{r}
# Evaluate joint pmf: 2,3
true_pmf = table(X[,2], X[,3])
true_pmf = c(true_pmf/sum(true_pmf))
imputed_pmf = table(X_POST[,2,], X_POST[,3,])
imputed_pmf = c(imputed_pmf/sum(imputed_pmf))
df = rbind(imputed_pmf, true_pmf)
barplot(df, xlab = 'Category', beside = TRUE, 
      legend = TRUE, main = paste('Posterior Predictive Distribution Assessment: joint pmf 2,3'),
      ylim = c(0,0.5))
```

```{r}
# Evaluate joint pmf: 1,3
true_pmf = table(X[,1], X[,3])
true_pmf = c(true_pmf/sum(true_pmf))
imputed_pmf = table(X_POST[,1,], X_POST[,3,])
imputed_pmf = c(imputed_pmf/sum(imputed_pmf))
df = rbind(imputed_pmf, true_pmf)
barplot(df, xlab = 'Category', beside = TRUE, 
      legend = TRUE, main = paste('Posterior Predictive Distribution Assessment: joint pmf 1,3'),
      ylim = c(0,0.5))
```
* * *



