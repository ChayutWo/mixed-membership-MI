
---
title: "DP for MI - Simulation example with 30% MCAR"
output:
  pdf_document: default
  html_document:
    highlight: pygments
    theme: spacelab
---

```{r setup, echo =FALSE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.pos = 'h', fig.align = 'center')
knitr::opts_chunk$set(fig.cap = "",  fig.path = "Plot")
library(knitr)
library(dplyr)
library(arm)
library(pROC)
library(tidyverse)
library(MASS)
library(tigerstats)
library(leaps)
library(car)
library(rms)
require(caret)
require(e1071)
library(lme4) 
library(lattice)
library(broom)
library(boot)
library(ggplot2)
library(cobalt)
require(tidyverse)
require(rstanarm)
require(magrittr)
require(rstan)
require(MCMCpack)
library(abind)
library(matrixStats)
library(truncnorm)
library(mvtnorm)
library(MCMCpack)
library(mnormt)
library(coda)
library(NPBayesImputeCat)
```

* * *
### Generate small dataset

Settings 
- N = 2000 surveyed subjects
- K = 3 clusters
- p = 3 questions/variables
- d1 = 2 choices while d2 = d3 = 3 choices
- $\gamma$ = 2 and $\alpha_0$ = 1 for DP generative process

Generative process
- $\beta|\gamma \sim GEM(\gamma)$
- $\pi_i|\alpha_0 \sim DP(\alpha_0, \beta)$ for i = 1,...,N
- $z_{ij}|\pi_i \sim Cat(\pi_i)$ for i = 1,...,N and j = 1,...,p
- $x_{ij}|z_{ij}=k \sim Cat(\Phi^{(j)}_k)$ for i = 1,...,N and j=1,...,p

```{r}
set.seed(1)
# parameter setup
N = 2000 # number of surveyed subjects
K_true = 3 # number of true clusteres
p = 3 # number of survey questions
d1 = 2 # number of levels of 1st variable
d2 = 3 # number of levels of 2st variable
d3 = 3 # number of levels of 3st variable
gamma_true = 2
alpha_true = 1
level = c(d1, d2, d3)

# place holder
Phi_1 = matrix(NA, nrow = K_true, ncol = d1) # multinomial parameter for 1st variable
Phi_2 = matrix(NA, nrow = K_true, ncol = d2) # multinomial parameter for 2nd variable
Phi_3 = matrix(NA, nrow = K_true, ncol = d3) # multinomial parameter for 3rd variable

```

```{r}
# Phi parameter for different clusters and different variables
# size #clusterx#levels

# 1st variable (d1 = 2)
Phi_1[,1] = c(0.1, 0.5, 0.9)
Phi_1[,2] = 1 - Phi_1[,1]

# 2nd variable (d2 = 3)
Phi_2[,1] = c(0.1, 0.1, 0.8)
Phi_2[,2] = c(0.1, 0.8, 0.1)
Phi_2[,3] = 1 - Phi_2[,1] - Phi_2[,2]

# 3rd variable (d3 = 3)
Phi_3[,1] = c(0.1, 0.1, 0.8)
Phi_3[,2] = c(0.8, 0.1, 0.1)
Phi_3[,3] = 1 - Phi_3[,1] - Phi_3[,2]

Phi = list(Phi_1, Phi_2, Phi_3)
```

```{r}
# Beta ~ GEM(gamma_true)
Beta_true = array(NA, dim = K_true)
V = array(NA, K_true)
V[K_true] = 1
prod = 1 # product of (1-V_l) for l<k
# Sample V_k from k = 1,...,K-1
for (k in 1:(K_true - 1)) {
  V[k] = rbeta(1, 1, gamma_true) # Vk ~ Beta(1, gamma)
  Beta_true[k] = V[k]*prod
  prod = prod*(1-V[k])
}
Beta_true[K_true] = 1-sum(Beta_true[1:(K_true-1)])
```

```{r}
# Sample cluster assignment z_i ~ Cat(Beta)
# Sample observation x_ij|z_i=k ~ Cat(Phi_jk)
z_true = sample(1:K_true, N, replace = TRUE, prob = Beta_true) # true cluster assignment
X = matrix(NA, nrow = N, ncol = p) # data matrix
for (i in 1:N) {
  z = z_true[i]
  for (j in 1:p) {
    prob = Phi[[j]][z,]
    X[i,j] = sample(1:length(prob), 1, replace = TRUE, prob = prob)
  }
}
```

```{r}
# make MCAR
missing_prob = 0.3
X_miss = X
for (col in 1:ncol(X)) {
    missing_ind <- rbernoulli(N, p = missing_prob)
    X_miss[missing_ind, col] <- NA
}
R = is.na(X_miss)
```

### Visualization

```{r}
barplot(table(z_true)/N, main = 'cluster assignment: z', ylim = c(0,1))
```

```{r}
# 1st variable
var = 1
barplot(table(X[,var])/N, main = 'overall')
for (z in 1:K_true) {
  barplot(table(X[z_true==z,var])/sum(table(X[z_true==z,var])), 
        main = paste('variable:',var,'/', 'cluster:',z), ylim = c(0,1))
}
```

```{r}
# 2nd variable
var = 2
barplot(table(X[,var])/N, main = 'overall')
for (z in 1:K_true) {
  barplot(table(X[z_true==z,var])/sum(table(X[z_true==z,var])), 
        main = paste('variable:',var,'/', 'cluster:',z), ylim = c(0,1))
}
```

```{r}
# 3rd variable
var = 3
barplot(table(X[,var])/N, main = 'overall')
for (z in 1:K_true) {
  barplot(table(X[z_true==z,var])/sum(table(X[z_true==z,var])), 
        main = paste('variable:',var,'/', 'cluster:',z), ylim = c(0,1))
}
```

### Dirichlet Process Mixture of Product of Multinomial MCMC

```{r}
set.seed(1)
# Convert data type to factor
data_df <- data.frame(X_miss)
data_df[,colnames(data_df)] <- lapply(data_df[,colnames(data_df)], factor)

# MCMC variables
Mon = 10000
burnin = 10000
thining = 50
K = 60
a = 0.25
b = 0.25
# 1. Create and initialize the Rcpp_Lcm model object
model = CreateModel(X = data_df, MCZ = NULL, K = K, Nmax = 0, 
                    aalpha = a, balpha = b)

# 2. Set tracer
model$SetTrace(c('k_star', 'psi', 'ImputedX', 'alpha'), Mon)
  
# 3. Run model using Run(burnin, iter, thinning)
model$Run(burnin, Mon, thining)
```



### Posterior Diagnostics


```{r}
X_POST = array(NA, c(N, p, Mon/thining))
# Extract results
output <- model$GetTrace()
k_star <- output$k_star
psi <- output$psi
alpha <- output$alpha
imputed_df <- output$ImputedX # size no. samples x (N*p)

n_imputations = Mon/thining
imputation_index = as.integer(seq(1,dim(imputed_df)[1], length.out = n_imputations))

for (i in 1:length(imputation_index)) {
    index = imputation_index[i]
    # need to plus 1 here because the class index of DP function starts at 0
    d = imputed_df[index,] + 1
    dim(d) = dim(t(data_df))
    X_POST[,,i] = t(d)
}
```


```{r}
alpha0.mcmc <- mcmc(alpha, start = 1)
summary(alpha0.mcmc)
plot(alpha0.mcmc)
autocorr.plot(alpha0.mcmc)
```


```{r}
# Check number of active cluster 
plot(1:length(k_star), k_star, xlab = 'trials', 
     ylab = 'number of active clusters', 
     main = 'Number of clusters used over time', ylim = c(0,K))
```


```{r}
# Compare true marginal distribution with posterior samples
for (var in 1:p) {
  true_pmf = table(X[,var])/N
  imputed_pmf = table(X_POST[,var,])
  imputed_pmf = imputed_pmf/sum(imputed_pmf)
  df = rbind(imputed_pmf, true_pmf)
  barplot(df, xlab = 'Category', beside = TRUE, 
        legend = TRUE, main = paste('Posterior Predictive Distribution Assessment: j =', var))
}
```

```{r}
# Evaluate joint pmf: 1,2
true_pmf = table(X[,1], X[,2])
true_pmf = c(true_pmf/sum(true_pmf))
imputed_pmf = table(X_POST[,1,], X_POST[,2,])
imputed_pmf = c(imputed_pmf/sum(imputed_pmf))
df = rbind(imputed_pmf, true_pmf)
barplot(df, xlab = 'Category', beside = TRUE, 
      legend = TRUE, main = paste('Posterior Predictive Distribution Assessment: joint pmf 1,2'),
      ylim = c(0,0.5))
```

```{r}
# Evaluate joint pmf: 2,3
true_pmf = table(X[,2], X[,3])
true_pmf = c(true_pmf/sum(true_pmf))
imputed_pmf = table(X_POST[,2,], X_POST[,3,])
imputed_pmf = c(imputed_pmf/sum(imputed_pmf))
df = rbind(imputed_pmf, true_pmf)
barplot(df, xlab = 'Category', beside = TRUE, 
      legend = TRUE, main = paste('Posterior Predictive Distribution Assessment: joint pmf 2,3'),
      ylim = c(0,0.5))
```

```{r}
# Evaluate joint pmf: 1,3
true_pmf = table(X[,1], X[,3])
true_pmf = c(true_pmf/sum(true_pmf))
imputed_pmf = table(X_POST[,1,], X_POST[,3,])
imputed_pmf = c(imputed_pmf/sum(imputed_pmf))
df = rbind(imputed_pmf, true_pmf)
barplot(df, xlab = 'Category', beside = TRUE, 
      legend = TRUE, main = paste('Posterior Predictive Distribution Assessment: joint pmf 1,3'),
      ylim = c(0,0.5))
```
* * *



