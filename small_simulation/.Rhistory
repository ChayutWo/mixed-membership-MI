true_pmf = c(true_pmf/sum(true_pmf))
sampling_pmf = table(X_predictive[,1,], X_predictive[,2,])
sampling_pmf = c(sampling_pmf/sum(sampling_pmf))
imputed_pmf = table(X_POST[,1,], X_POST[,2,])
imputed_pmf = c(imputed_pmf/sum(imputed_pmf))
df = rbind(sampling_pmf, true_pmf, imputed_pmf)
barplot(df, xlab = 'Category', beside = TRUE,
legend = TRUE, main = paste('Posterior Predictive Distribution Assessment: joint pmf 1,2'),
ylim = c(0,0.5))
# Evaluate joint pmf: 1,2
true_pmf = table(X[,2], X[,3])
true_pmf = c(true_pmf/sum(true_pmf))
sampling_pmf = table(X_predictive[,2,], X_predictive[,3,])
sampling_pmf = c(sampling_pmf/sum(sampling_pmf))
imputed_pmf = table(X_POST[,2,], X_POST[,3,])
imputed_pmf = c(imputed_pmf/sum(imputed_pmf))
df = rbind(sampling_pmf, true_pmf, imputed_pmf)
barplot(df, xlab = 'Category', beside = TRUE,
legend = TRUE, main = paste('Posterior Predictive Distribution Assessment: joint pmf 2,3'),
ylim = c(0,0.5))
# Evaluate joint pmf: 1,2
true_pmf = table(X[,1], X[,3])
true_pmf = c(true_pmf/sum(true_pmf))
sampling_pmf = table(X_predictive[,1,], X_predictive[,3,])
sampling_pmf = c(sampling_pmf/sum(sampling_pmf))
imputed_pmf = table(X_POST[,1,], X_POST[,3,])
imputed_pmf = c(imputed_pmf/sum(imputed_pmf))
df = rbind(sampling_pmf, true_pmf, imputed_pmf)
barplot(df, xlab = 'Category', beside = TRUE,
legend = TRUE, main = paste('Posterior Predictive Distribution Assessment: joint pmf 1,3'),
ylim = c(0,0.5))
knitr::opts_chunk$set(echo = TRUE, fig.pos = 'h', fig.align = 'center')
knitr::opts_chunk$set(fig.cap = "",  fig.path = "Plot")
library(knitr)
library(dplyr)
library(arm)
library(pROC)
library(tidyverse)
library(MASS)
library(tigerstats)
library(leaps)
library(car)
library(rms)
require(caret)
require(e1071)
library(lme4)
library(lattice)
library(broom)
library(boot)
library(ggplot2)
library(cobalt)
require(tidyverse)
require(rstanarm)
require(magrittr)
require(rstan)
require(MCMCpack)
library(abind)
library(matrixStats)
library(truncnorm)
library(mvtnorm)
library(MCMCpack)
library(mnormt)
library(coda)
set.seed(1)
# parameter setup
N = 2000 # number of surveyed subjects
K_true = 3 # number of true clusteres
p = 3 # number of survey questions
d1 = 2 # number of levels of 1st variable
d2 = 3 # number of levels of 2st variable
d3 = 3 # number of levels of 3st variable
gamma_true = 2
alpha_true = 1
level = c(d1, d2, d3)
# place holder
Phi_1 = matrix(NA, nrow = K_true, ncol = d1) # multinomial parameter for 1st variable
Phi_2 = matrix(NA, nrow = K_true, ncol = d2) # multinomial parameter for 2nd variable
Phi_3 = matrix(NA, nrow = K_true, ncol = d3) # multinomial parameter for 3rd variable
# Phi parameter for different clusters and different variables
# size #clusterx#levels
# 1st variable (d1 = 2)
Phi_1[,1] = c(0.1, 0.5, 0.9)
Phi_1[,2] = 1 - Phi_1[,1]
# 2nd variable (d2 = 3)
Phi_2[,1] = c(0.1, 0.1, 0.8)
Phi_2[,2] = c(0.1, 0.8, 0.1)
Phi_2[,3] = 1 - Phi_2[,1] - Phi_2[,2]
# 3rd variable (d3 = 3)
Phi_3[,1] = c(0.1, 0.1, 0.8)
Phi_3[,2] = c(0.8, 0.1, 0.1)
Phi_3[,3] = 1 - Phi_3[,1] - Phi_3[,2]
Phi = list(Phi_1, Phi_2, Phi_3)
# Beta ~ GEM(gamma_true)
Beta_true = array(NA, dim = K_true)
V = array(NA, K_true)
V[K_true] = 1
prod = 1 # product of (1-V_l) for l<k
# Sample V_k from k = 1,...,K-1
for (k in 1:(K_true - 1)) {
V[k] = rbeta(1, 1, gamma_true) # Vk ~ Beta(1, gamma)
Beta_true[k] = V[k]*prod
prod = prod*(1-V[k])
}
Beta_true[K_true] = 1-sum(Beta_true[1:(K_true-1)])
# Pi_i ~ DP(alpha_true, Beta_true) for i = 1,...,N
Pi_true = matrix(NA, N, K_true)
u = matrix(NA, N, K_true)
u[,K_true] = 1
prod = array(1, N) # product of (1-u_il) for l<k
# sample u_ik ~ Beta distribution
# then, obtain Pi from u
for (k in 1:(K_true - 1)) {
u[,k] = rbeta(N, alpha_true*Beta_true[k], alpha_true*(1-sum(Beta_true[1:k])))
Pi_true[,k] = u[,k]*prod
prod = prod*(1-u[,k])
}
Pi_true[,K_true] = 1-rowSums(Pi_true[,1:(K_true-1)])
# Check the mean of Pi from DP
cat('Beta:', Beta_true,'\n')
cat('Pi:', apply(Pi_true, MARGIN = 2, FUN = mean))
# Sample cluster assignment z_ij ~ Cat(Pi_i)
# Sample observation x_ij|z_ij=k ~ Cat(Phi_jk)
z_true = matrix(NA, nrow = N, ncol = p) # true cluster assignment
X = matrix(NA, nrow = N, ncol = p) # data matrix
for (i in 1:N) {
z_true[i,] = sample(1:K_true, p, replace=TRUE, prob=Pi_true[i,])
for (j in 1:p) {
z = z_true[i,j]
prob = Phi[[j]][z,]
X[i,j] = sample(1:length(prob), 1, replace = TRUE, prob = prob)
}
}
# make MCAR
missing_prob = 0.2
X_miss = X
for (col in 1:ncol(X)) {
missing_ind <- rbernoulli(N, p = missing_prob)
X_miss[missing_ind, col] <- NA
}
R = is.na(X_miss)
# initial imputation
for (col in 1:ncol(X)) {
missing_ind <- R[,col]
X_miss[missing_ind, col] <- sample(1:level[col], size = sum(missing_ind),
table(X_miss[R[,col]!=1,col]), replace = TRUE)
}
barplot(table(z_true)/N/p, main = 'cluster assignment: z', ylim = c(0,1))
# 1st variable
var = 1
barplot(table(X[,var])/N, main = 'overall')
for (z in 1:K_true) {
barplot(table(X[z_true[,var]==z,var])/sum(table(X[z_true[,var]==z,var])),
main = paste('variable:',var,'/', 'cluster:',z), ylim = c(0,1))
}
# 2nd variable
var = 2
barplot(table(X[,var])/N, main = 'overall')
for (z in 1:K_true) {
barplot(table(X[z_true[,var]==z,var])/sum(table(X[z_true[,var]==z,var])),
main = paste('variable:',var,'/', 'cluster:',z), ylim = c(0,1))
}
# 2nd variable
var = 3
barplot(table(X[,var])/N, main = 'overall')
for (z in 1:K_true) {
barplot(table(X[z_true[,var]==z,var])/sum(table(X[z_true[,var]==z,var])),
main = paste('variable:',var,'/', 'cluster:',z), ylim = c(0,1))
}
set.seed(10)
# MCMC variables
Mon = 100
burnin = 5
N = dim(X)[1]
K = 10
p = dim(X)[2]
# hyperprior parameters
# gamma ~ Gamma(a,b)
a = 0.25
b = 0.25
# alpha0 ~ Gamma(c,d)
c = 0.25
d = 0.25
# storage
GAMMA = rep(NA, Mon-burnin)
ALPHA0 = rep(NA, Mon-burnin)
BETA = matrix(NA, nrow = Mon-burnin, ncol = K)
PI = array(NA, c(N, K, Mon-burnin))
Z = array(NA, c(N, p, Mon-burnin))
X_SAMPLE = array(NA, c(N, p, Mon-burnin))
PHI1 = array(NA, c(K, d1, Mon-burnin))
PHI2 = array(NA, c(K, d2, Mon-burnin))
PHI3 = array(NA, c(K, d3, Mon-burnin))
# initialization for x
# x_temp = X
x_temp = X_miss
# initialization for z
z_temp = matrix(sample(1:K, size = N*p, replace = TRUE), nrow = N, ncol = p)
# nik = # of variables in cluster k for observation i
nik = matrix(NA, N, K)
for (k in 1:K) {
nik[,k] = rowSums(z_temp==k)
}
# initialization for PHI
phi_temp = list()
for (j in 1:p) {
prob = rdirichlet(n = K, rep(1,level[j])) # Kxlevel matrix
phi_temp[[j]] = prob
}
# initialization for gamma and alpha0
gamma_temp = 1
alpha0_temp = 1
# initialization for Vk and Betak
# Beta ~ GEM(gamma_temp)
beta_temp = array(NA, dim = K)
Vk_temp = array(NA, K)
Vk_temp[K] = 1
prod = 1 # product of (1-V_l) for l<k
# Sample V_k from k = 1,...,K-1
for (k in 1:(K - 1)) {
Vk_temp[k] = rbeta(1, 1, gamma_temp)
# Prevent instability issue
if (Vk_temp[k]==0) Vk_temp[k] = 0.00001
if (Vk_temp[k]==1) Vk_temp[k] = 0.9999
beta_temp[k] = Vk_temp[k]*prod
prod = prod*(1-Vk_temp[k])
}
# Prevent instability issue
if (1-sum(beta_temp[1:(K-1)])>=0) {
beta_temp[K] = 1-sum(beta_temp[1:(K-1)])
}else{
beta_temp[K] = 0
}
# initialization for u and Pi
# Pi_i ~ DP(alpha_temp, Beta_temp) for i = 1,...,N
Pi_temp = matrix(NA, N, K)
u_temp = matrix(NA, N, K)
u_temp[,K] = 1
prod = array(1, N) # product of (1-u_il) for l<k
# sample u_ik ~ Beta distribution
# then, obtain Pi from u
for (k in 1:(K - 1)) {
u_temp[,k] = rbeta(N, alpha0_temp*beta_temp[k], alpha0_temp*(1-sum(beta_temp[1:k])))
# Prevent instability issue
idx = (u_temp[,k] == 1)
u_temp[idx, k] = 0.9999
idx = (u_temp[,k] == 0)
u_temp[idx, k] = 0.00001
Pi_temp[,k] = u_temp[,k]*prod
prod = prod*(1-u_temp[,k])
}
Pi_temp[,K] = 1-rowSums(Pi_temp[,1:(K-1)])
# Prevent instability issue
idx = (Pi_temp[,K] < 0)
Pi_temp[idx,K] = 0
# initialization for auxiliary variables
t_temp = array(NA, N)
sik = matrix(0, N, K)
for (trial in 1:Mon) {
# Step 1: Update z_ij
for (j in 1:p) {
# get phi_j
prob = phi_temp[[j]]
var = x_temp[,j]
likelihood = t(prob[,var]) # NxK of phi_j_xij
full_prob_matrix = Pi_temp*likelihood
full_prob_matrix <- full_prob_matrix/matrix(rowSums(full_prob_matrix),nrow=N,ncol=K) # normalize
Ran_unif <- runif(N)
cumul <- full_prob_matrix%*%upper.tri(diag(ncol(full_prob_matrix)),diag=TRUE)
z_temp[,j] <- rowSums(Ran_unif>cumul) + 1L
}
# update nik
nik = matrix(NA, N, K)
for (k in 1:K) {
nik[,k] = rowSums(z_temp==k)
}
# Step 2: Update Phi_k
for (j in 1:p) {
for (k in 1:K) {
dj = level[j]
njk = rep(NA, dj) # number of response j in cluster k that answer different levels
for (lev in 1:dj) {
# number of response j that has level lev and under cluster k
njk[lev] = sum(x_temp[,j]==lev & z_temp[,j]==k)
}
phi_temp[[j]][k,] = rdirichlet(1,rep(1,level[j])+njk)
}
}
# Step 3: Update Vk and betak
# sample auxiliary variable ti
t_temp = array(NA, N)
sik = matrix(0, N, K)
for (i in 1:N) {
t_temp[i] = rbeta(1,alpha0_temp,sum(nik[i,]))
# sample auxiliary variable sik
for (k in 1:K) {
if (nik[i,k]==0) {
sik[i,k]=0
}else{
for (l in 1:(nik[i,k])) {
prob = (alpha0_temp*beta_temp[k])/(alpha0_temp*beta_temp[k]+l-1)
sik[i,k] = sik[i,k]+rbernoulli(1,prob)
}
}
}
}
Vk_temp[K] = 1
prod = 1
# Sample V_k from k = 1,...,K-1
for (k in 1:(K - 1)) {
Vk_temp[k] = rbeta(1, 1 + sum(sik[,k]), gamma_temp + sum(sik[,(k+1):K]))
# Prevent instability issue
if (Vk_temp[k]==0) Vk_temp[k] = 0.00001
if (Vk_temp[k]==1) Vk_temp[k] = 0.9999
# updaet beta_k
beta_temp[k] = Vk_temp[k]*prod
prod = prod*(1-Vk_temp[k])
}
# Prevent instability issue
if (1-sum(beta_temp[1:(K-1)])>=0) {
beta_temp[K] = 1-sum(beta_temp[1:(K-1)])
}else{
beta_temp[K] = 0
}
if (sum(beta_temp<0)>0) {
print('error with beta < 0')
}
# Step 4: Update u_ik and pi_ik
prod = array(1, N) # product of (1-u_il) for l<k
for (k in 1:(K-1)) {
# shape factors for posterior distribution of u
param1 = alpha0_temp*beta_temp[k] + nik[,k]
param2 = alpha0_temp*(1-sum(beta_temp[1:k])) + rowSums(z_temp > k)
# update u
u_temp[,k] = rbeta(N, shape1 = param1, shape2 = param2)
# Prevent instability issue
idx = (u_temp[,k] == 1)
u_temp[idx, k] = 0.9999
idx = (u_temp[,k] == 0)
u_temp[idx, k] = 0.00001
# update Pi
Pi_temp[,k] = u_temp[,k]*prod
prod = prod*(1-u_temp[,k])
}
Pi_temp[,K] = 1-rowSums(Pi_temp[,1:(K-1)])
# Prevent instability issue
idx = (Pi_temp[,K] < 0)
Pi_temp[idx,K] = 0
# Step 5.1: Update gamma
gamma_temp = rgamma(1, a + K - 1, b - sum(log(1-Vk_temp[1:(K-1)])))
# Step 5.2: Update alpha
alpha0_temp = rgamma(1, c + sum(sik), d - sum(log(t_temp)))
# Step 6: Update x_ij
for (i in 1:N) {
for (j in 1:p) {
if (R[i,j] == 1) {
# if the value is missing, sample from posterior discrete distribution
z_cur = z_temp[i,j]
prob = phi_temp[[j]][z_cur,]
prob_cum = prob%*%upper.tri(diag(length(prob)),diag=TRUE)
Ran_unif <- runif(1)
x_temp[i,j] <- sum(Ran_unif>prob_cum) + 1L
}
}
}
# Save posterior samples
if (trial>burnin) {
id = trial-burnin
GAMMA[id] = gamma_temp
ALPHA0[id] = alpha0_temp
BETA[id,] = beta_temp
PI[,,id] = Pi_temp
Z[,,id] = z_temp
X_SAMPLE[,,id] = x_temp
PHI1[,,id] = phi_temp[[1]]
PHI2[,,id] = phi_temp[[2]]
PHI3[,,id] = phi_temp[[3]]
}
print(paste('finish runing MCMC trial:',trial))
}
thining = 25
gamma.mcmc <- mcmc(GAMMA[seq(1, Mon-burnin, thining)], start = 1)
summary(gamma.mcmc)
plot(gamma.mcmc)
autocorr.plot(gamma.mcmc)
alpha0.mcmc <- mcmc(ALPHA0[seq(1, Mon-burnin, thining)], start = 1)
summary(alpha0.mcmc)
plot(alpha0.mcmc)
autocorr.plot(alpha0.mcmc)
# Convergence checking Beta a after accounting for burnin and thining
beta_df = BETA[seq(1, Mon-burnin, thining),]
matplot(1:dim(beta_df)[1], beta_df,
type = 'l', ylab = 'Beta_k', xlab = 'trials',
main = 'Checking stability of sampled Beta: thining=5')
# Check number of active cluster (not thining)
Z_df = Z
active_cluster = array(0, dim(Z_df)[3])
for (idx in 1:dim(active_cluster)) {
for (k in 1:K) {
if (sum(Z_df[,,idx]==k)!=0) {
active_cluster[idx] = active_cluster[idx] + 1
}
}
}
plot(1:dim(active_cluster), active_cluster, type = 'l', ylab = 'Number of active clusters',
xlab = 'trials', main = 'Checking number of active clusters during MCMC', ylim = c(1,K))
# Convergence checking for Phi1
for (k in 1:K) {
phi_df = cbind(PHI1[k,1,seq(1, Mon-burnin, thining)],
PHI1[k,2,seq(1, Mon-burnin, thining)])
colnames(phi_df) = c('d=1', 'd=2')
matplot(1:dim(phi_df)[1], phi_df,
type = 'l', ylab = 'phi', xlab = 'trials',
main = paste('Checking stability of sampled phi1: K=',k), ylim = c(0,1),
col = c('black', 'green'), lty = 1:2, lwd = 1)
legend('right', legend = colnames(phi_df), col = c('black', 'green'), lty = 1:2, lwd = 1)
}
# Convergence checking for Phi2
for (k in 1:K) {
phi_df = cbind(PHI2[k,1,seq(1, Mon-burnin, thining)],
PHI2[k,2,seq(1, Mon-burnin, thining)],
PHI2[k,3,seq(1, Mon-burnin, thining)])
colnames(phi_df) = c('d=1', 'd=2', 'd=3')
matplot(1:dim(phi_df)[1], phi_df,
type = 'l', ylab = 'phi', xlab = 'trials',
main = paste('Checking stability of sampled phi2: K=',k), ylim = c(0,1),
col = c('black', 'green', 'red'), lty = 1:3, lwd = 1)
legend('right', legend = colnames(phi_df), col = c('black', 'green', 'red'), lty = 1:3, lwd = 1)
}
# Convergence checking for Phi3
for (k in 1:K) {
phi_df = cbind(PHI3[k,1,seq(1, Mon-burnin, thining)],
PHI3[k,2,seq(1, Mon-burnin, thining)],
PHI3[k,3,seq(1, Mon-burnin, thining)])
colnames(phi_df) = c('d=1', 'd=2', 'd=3')
matplot(1:dim(phi_df)[1], phi_df,
type = 'l', ylab = 'phi', xlab = 'trials',
main = paste('Checking stability of sampled phi3: K=',k), ylim = c(0,1),
col = c('black', 'green', 'red'), lty = 1:3, lwd = 1)
legend('right', legend = colnames(phi_df), col = c('black', 'green', 'red'), lty = 1:3, lwd = 1)
}
# Evaluate posterior predictive distribution
Z_predictive = array(NA, c(N,p,length(seq(1, Mon-burnin, thining))))
X_predictive = array(NA, c(N,p,length(seq(1, Mon-burnin, thining))))
for (idx in 1:dim(Z_predictive)[3]) {
for (i in 1:N) {
# Sample predictive distribution: Z
prob = PI[i,,seq(1, Mon-burnin, thining)[idx]]
Z_predictive[i,,idx] = sample(1:K, p, replace=TRUE, prob)
for (j in 1:p) {
z = Z_predictive[i,j,idx]
if (j==1) prob = PHI1[z,,seq(1, Mon-burnin, thining)[idx]]
if (j==2) prob = PHI2[z,,seq(1, Mon-burnin, thining)[idx]]
if (j==3) prob = PHI3[z,,seq(1, Mon-burnin, thining)[idx]]
# Sample predictive distribution: X
X_predictive[i,j,idx] = sample(1:length(prob), size = 1, replace = TRUE,  prob)
}
}
print(paste('finish sampling: ', idx))
}
# Compare predictive distribution with true marginal distribution and posterior samples
X_POST = X_SAMPLE[,,seq(1, Mon-burnin, thining)]
for (var in 1:p) {
true_pmf = table(X[,var])/N
sampling_pmf = table(X_predictive[,var,])
sampling_pmf = sampling_pmf/sum(sampling_pmf)
imputed_pmf = table(X_POST[,var,])
imputed_pmf = imputed_pmf/sum(imputed_pmf)
df = rbind(sampling_pmf, imputed_pmf, true_pmf)
barplot(df, xlab = 'Category', beside = TRUE,
legend = TRUE, main = paste('Posterior Predictive Distribution Assessment: j =', var))
}
# Evaluate joint pmf: 1,2
true_pmf = table(X[,1], X[,2])
true_pmf = c(true_pmf/sum(true_pmf))
sampling_pmf = table(X_predictive[,1,], X_predictive[,2,])
sampling_pmf = c(sampling_pmf/sum(sampling_pmf))
imputed_pmf = table(X_POST[,1,], X_POST[,2,])
imputed_pmf = c(imputed_pmf/sum(imputed_pmf))
df = rbind(sampling_pmf, imputed_pmf, true_pmf)
barplot(df, xlab = 'Category', beside = TRUE,
legend = TRUE, main = paste('Posterior Predictive Distribution Assessment: joint pmf 1,2'),
ylim = c(0,0.5))
# Evaluate joint pmf: 1,2
true_pmf = table(X[,2], X[,3])
true_pmf = c(true_pmf/sum(true_pmf))
sampling_pmf = table(X_predictive[,2,], X_predictive[,3,])
sampling_pmf = c(sampling_pmf/sum(sampling_pmf))
imputed_pmf = table(X_POST[,2,], X_POST[,3,])
imputed_pmf = c(imputed_pmf/sum(imputed_pmf))
df = rbind(sampling_pmf, imputed_pmf, true_pmf)
barplot(df, xlab = 'Category', beside = TRUE,
legend = TRUE, main = paste('Posterior Predictive Distribution Assessment: joint pmf 2,3'),
ylim = c(0,0.5))
# Evaluate joint pmf: 1,2
true_pmf = table(X[,1], X[,3])
true_pmf = c(true_pmf/sum(true_pmf))
sampling_pmf = table(X_predictive[,1,], X_predictive[,3,])
sampling_pmf = c(sampling_pmf/sum(sampling_pmf))
imputed_pmf = table(X_POST[,1,], X_POST[,3,])
imputed_pmf = c(imputed_pmf/sum(imputed_pmf))
df = rbind(sampling_pmf, imputed_pmf, true_pmf)
barplot(df, xlab = 'Category', beside = TRUE,
legend = TRUE, main = paste('Posterior Predictive Distribution Assessment: joint pmf 1,3'),
ylim = c(0,0.5))
dim(X_POS)
dim(X_POST)
dim(X_SAMPLE)
X_1 = X_SAMPLE[,,5]
View(X_1)
View(X)
View(X_miss)
View(R)
