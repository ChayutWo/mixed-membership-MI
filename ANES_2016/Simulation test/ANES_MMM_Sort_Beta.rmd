
---
title: "ANES dataset model with MMM without missing values - fix label switching"
output:
  pdf_document: default
  html_document:
    highlight: pygments
    theme: spacelab
---

```{r setup, echo =FALSE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.pos = 'h', fig.align = 'center')
knitr::opts_chunk$set(fig.cap = "",  fig.path = "Plot")
library(knitr)
library(dplyr)
library(arm)
library(pROC)
library(tidyverse)
library(MASS)
library(tigerstats)
library(leaps)
library(car)
library(rms)
require(caret)
require(e1071)
library(lme4) 
library(lattice)
library(broom)
library(boot)
library(ggplot2)
library(cobalt)
require(tidyverse)
require(rstanarm)
require(magrittr)
require(rstan)
require(MCMCpack)
library(abind)
library(matrixStats)
library(truncnorm)
library(mvtnorm)
library(MCMCpack)
library(mnormt)
library(coda)
library(NPBayesImputeCat)
library(pheatmap)
```

* * *
```{r}
# here is the code to generate final dataset to be used in modelling
filepath = '../anes_timeseries_2016/anes_timeseries_2016_rawdata.txt'
X = read.csv(filepath,sep= '|')
# select variables: 26 variables
var_name = c('V162123', 'V162134', 'V162140', 'V162145', 'V162146',
             'V162148', 'V162158', 'V162170', 'V162176', 'V162179',
             'V162180', 'V162192', 'V162193', 'V162207', 'V162208',
             'V162209', 'V162212', 'V162214', 'V162231', 'V162233', 
             'V162246', 'V162260', 'V162265', 'V162269', 'V162271', 
             'V162290')
X = X[, var_name]
# extract complete cases
complete_ind = (apply(X <= 0,MARGIN = 1, sum) == 0)
X = X[complete_ind,]
# fix problem with the lebel of V162290: levels 1,2,4,5 to 1,2,3,4
ind_4 = X[,'V162290']==4
X[ind_4,'V162290'] = 3
ind_5 = X[,'V162290']==5
X[ind_5,'V162290'] = 4
# Merge level 3 and 4 from V162192
ind_4 = X[,'V162192']==4
X[ind_4,'V162192'] = 3
# format data into categorical variables
level = apply(X, MARGIN=2, max)
X = data.frame(X)
for (col_index in 1:ncol(X)) {
  X[,col_index] = factor(X[,col_index], levels = 1:level[col_index])
}
```

```{r}
# Make missing data
X_miss = X
R = is.na(X_miss)
```

### Mixed-Membership Model (MMM) MCMC

```{r}
set.seed(1)
# MCMC variables
Mon = 15000
burnin = 5000
thining = 5
K = 30

N = dim(X)[1]
p = dim(X)[2]
# hyperprior parameters
# gamma ~ Gamma(a,b)
a = 0.25
b = 0.25
# alpha0 ~ Gamma(c,d)
c = 0.25
d = 0.25
# storage
GAMMA = rep(NA, (Mon-burnin)/thining)
ALPHA0 = rep(NA, (Mon-burnin)/thining)
BETA = matrix(NA, nrow = (Mon-burnin)/thining, ncol = K)
PI = array(NA, c(N, K, (Mon-burnin)/thining))
Z = array(NA, c(N, p, (Mon-burnin)/thining))
X_SAMPLE = array(NA, c(N, p, (Mon-burnin)/thining))
PHI = list()
for (j in 1:p) {
  PHI[[j]] = array(NA, c(K, level[j], (Mon-burnin)/thining))
}
```

```{r}
# initialization for x
x_temp = X_miss

# initialization for z
z_temp = data.frame(matrix(sample(1:K, size = N*p, replace = TRUE), nrow = N, ncol = p))
for (col_index in 1:ncol(z_temp)) {
  z_temp[,col_index] = factor(z_temp[,col_index], levels = 1:K)
}

# nik = # of variables in cluster k for observation i
nik = matrix(NA, N, K)
for (k in 1:K) {
  nik[,k] = rowSums(z_temp==k)
}
# initialization for PHI
phi_temp = list()
for (j in 1:p) {
  prob = rdirichlet(n = K, rep(1,level[j])) # Kxlevel matrix
  phi_temp[[j]] = prob
}
# initialization for gamma and alpha0
gamma_temp = 1
alpha0_temp = 1 

# initialization for Vk and Betak
# Beta ~ GEM(gamma_temp)
# Sample Vk ~ Beta(1, gamma)
Vk_temp <- matrix(rbeta(K-1,1,gamma_temp),nrow=K-1)
Vk_temp <- rbind(Vk_temp,1)
one_min_Vk_temp <- 1L-Vk_temp
one_min_Vk_temp <- c(1,cumprod(one_min_Vk_temp[1:(K-1)]))
# Compute Betak for k = 1,...,K
beta_temp <- Vk_temp*one_min_Vk_temp
# Sort Betak to fix label switching
beta_temp <- sort(beta_temp, decreasing = TRUE)

# initialization for u and Pi
# Pi_i ~ DP(alpha_temp, Beta_temp) for i = 1,...,N
one_min_cumsum_beta_temp <-1L-cumsum(beta_temp[1:(K-1)])
one_min_cumsum_beta_temp[one_min_cumsum_beta_temp<=0] <- 1.0e-20
u_temp <- matrix(rbeta(N*(K-1),alpha0_temp*rep(beta_temp[1:(K-1)],each=N),
                       alpha0_temp*rep(one_min_cumsum_beta_temp,each=N) ),
                 nrow=N,ncol=(K-1),byrow = F)
# Prevent instability issue
u_temp[u_temp==1] = 0.99999; u_temp[u_temp==0] = 1.0e-5
u_temp <- cbind(u_temp,1)
one_min_u_temp <- 1L-u_temp
one_min_u_temp <- cbind(1,t(apply(one_min_u_temp[,-K],1,cumprod)))
# Compute Pi from uik
Pi_temp <- u_temp*one_min_u_temp

# initialization for auxiliary variables
t_temp = array(NA, N)
sik = matrix(0, N, K)

# matrix used in MCMC
nik = matrix(NA, N, K)
```

```{r}
id = 0
for (trial in 1:Mon) {
  # Step 1: Update z_ij
  for (j in 1:p) {
    # get phi_j
    prob = phi_temp[[j]]
    var = x_temp[,j]
    likelihood = t(prob[,var]) # NxK of phi_j_xij
    full_prob_matrix = Pi_temp*likelihood
    full_prob_matrix <- full_prob_matrix/matrix(rowSums(full_prob_matrix),nrow=N,ncol=K) # normalize
    Ran_unif <- runif(N)
    cumul <- full_prob_matrix%*%upper.tri(diag(ncol(full_prob_matrix)),diag=TRUE)
    z_temp[,j] <- factor(rowSums(Ran_unif>cumul) + 1L, levels = 1:K)
  }
  
  # Step 2: Update Phi_k
  for (j in 1:p) {
    contingency_table = table(z_temp[,j], x_temp[,j])
    phi_temp[[j]] = DirichletReg::rdirichlet(K,1+contingency_table)
  }
  
  # Step 3: Update Vk and betak
  # sample auxiliary variable ti
  sik = matrix(0, N, K)
  for (k in 1:K){
    # update count nik of number of variables for each person in different clusters
    nik[,k] = rowSums(z_temp==k) 
    if(sum(nik[,k]>0)>0){
      # sample sik for only k where nk not equal 0
      non_zero_index <- which(nik[,k]>0)
      non_zero_nik <- nik[non_zero_index,k]
      sik[non_zero_index,k] <- 
        unlist(lapply(1:length(non_zero_nik), 
                      function(x) sum(rbernoulli(non_zero_nik[x], 
                                                ((alpha0_temp*beta_temp[k])/(alpha0_temp*beta_temp[k] + (1:non_zero_nik[x]) -1)) )) ))
    }
  }
  t_temp = rbeta(N,alpha0_temp,rowSums(nik))  
  
  # sample Vk for k = 1,...,K-1
  Vk_temp[1:(K-1)] <- rbeta((K-1),(1L+colSums(sik[,1:(K-1)])),
                            (gamma_temp + (sum(sik) - cumsum(colSums(sik)))[1:K-1]) )
  # Prevent instability issue
  Vk_temp[Vk_temp==1] <- 0.99999; Vk_temp[Vk_temp==0] <- 1.0e-5
  Vk_temp[K] <- 1
  one_min_Vk_temp <- 1L-Vk_temp
  one_min_Vk_temp <- c(1,cumprod(one_min_Vk_temp[1:(K-1)]))
  # compute betak for k = 1,...,K
  beta_temp <- Vk_temp*one_min_Vk_temp
  # Sort Betak to fix label switching
  beta_temp <- sort(beta_temp, decreasing = TRUE)
  
  # Step 4: Update u_ik and pi_ik
  one_min_cumsum_beta_temp <- 1L-cumsum(beta_temp[1:(K-1)])
  # Prevent instability issue
  one_min_cumsum_beta_temp[one_min_cumsum_beta_temp<=0] <- 1.0e-20
  u_temp[,1:(K-1)] <- matrix(rbeta( (N*(K-1)), (alpha0_temp*rep(beta_temp[1:(K-1)],each=N) + c(nik[,1:(K-1)]) ) ,
                         (alpha0_temp*rep(one_min_cumsum_beta_temp,each=N) +
                            c(matrix(rowSums(nik),ncol=K-1,nrow=N)-t(apply(nik,1,cumsum))[,1:K-1]) ) ),
                         nrow=N,ncol=(K-1),byrow = F)
  # Prevent instability issue
  u_temp[u_temp==1] = 0.99999; u_temp[u_temp==0] = 1.0e-5
  u_temp[,K] <- 1
  one_min_u_temp <- 1L-u_temp
  one_min_u_temp <- cbind(1,t(apply(one_min_u_temp[,-K],1,cumprod)))
  Pi_temp <- u_temp*one_min_u_temp
  
  # Step 5.1: Update gamma
  gamma_temp = rgamma(1, a + K - 1, b - sum(log(1-Vk_temp[1:(K-1)])))
  # Step 5.2: Update alpha
  alpha0_temp = rgamma(1, c + sum(sik), d - sum(log(t_temp)))
  
  # Step 6: Update x_ij
  if (FALSE) {
    # we dont have missing values
    for (j in 1:p) {
      # get z_ij for missing entries
      missing_idx = (R[,j] == 1)
      z_cur = z_temp[missing_idx,j]
      # get phi_ij
      prob = phi_temp[[j]][z_cur,] # number of missing entries x dj matrix
      prob_cum = prob%*%upper.tri(diag(ncol(prob)),diag=TRUE)
      Ran_unif <- runif(sum(missing_idx))
      x_temp[missing_idx, j] = rowSums(Ran_unif>prob_cum) + 1L
    }
  }

  
  # Save posterior samples
  if ((trial>burnin) && ((trial-burnin)%%thining==1) ) {
    id = id + 1
    GAMMA[id] = gamma_temp
    ALPHA0[id] = alpha0_temp
    BETA[id,] = beta_temp
    PI[,,id] = Pi_temp
    Z[,,id] = data.matrix(z_temp)
    #X_SAMPLE[,,id] = data.matrix(x_temp)
    for (j in 1:p) {
      PHI[[j]][,,id] = phi_temp[[j]]
    }
  }
  print(paste('finish runing MCMC trial:',trial))
}
```

### Posterior Diagnostics

```{r, echo = FALSE}
gamma.mcmc <- mcmc(GAMMA, start = 1)
summary(gamma.mcmc)
plot(gamma.mcmc)
autocorr.plot(gamma.mcmc)
```

```{r}
alpha0.mcmc <- mcmc(ALPHA0, start = 1)
summary(alpha0.mcmc)
plot(alpha0.mcmc)
autocorr.plot(alpha0.mcmc)
```

```{r}
# Convergence checking Beta a after accounting for burnin and thining
beta_df = BETA
colnames(beta_df) <- 1:K
matplot(1:dim(beta_df)[1], beta_df, 
        type = 'l', ylab = 'Beta_k', xlab = 'trials', 
        main = 'Checking stability of sampled Beta')

```

```{r}
# Check number of active cluster 
Z_df = Z
active_cluster = array(0, dim(Z_df)[3])
for (idx in 1:dim(active_cluster)) {
  for (k in 1:K) {
    if (sum(Z_df[,,idx]==k)!=0) {
      active_cluster[idx] = active_cluster[idx] + 1
    }
  }
}
plot(1:dim(active_cluster), active_cluster, type = 'l', ylab = 'Number of active clusters',
     xlab = 'trials', main = 'Checking number of active clusters during MCMC', ylim = c(1,K))

```

```{r, fig.width=3, fig.height=2}
# Convergence checking for V162123
for (k in 1:K) {
  phi_df = cbind(PHI[[1]][k,1,], 
                 PHI[[1]][k,2,],
                 PHI[[1]][k,3,],
                 PHI[[1]][k,4,],
                 PHI[[1]][k,5,])
  colnames(phi_df) = c('d=1', 'd=2', 'd=3', 'd=4', 'd=5')
  matplot(1:dim(phi_df)[1], phi_df, 
          type = 'l', ylab = 'phi', xlab = 'trials', 
          main = paste('V162123 Better if rest of world more like US: K=',k), ylim = c(0,1),
          col = c('black', 'green', 'blue', 'red', 'pink'), lty = 1:5, lwd = 1, cex.main = 0.75)
  legend('right', legend = colnames(phi_df), col = c('black', 'green', 'blue', 'red', 'pink'), lty = 1:5, lwd = 1)
}
```

```{r, fig.width=3, fig.height=2}
# Convergence checking for V162209
for (k in 1:K) {
  phi_df = cbind(PHI[[16]][k,1,], 
                 PHI[[16]][k,2,],
                 PHI[[16]][k,3,],
                 PHI[[16]][k,4,],
                 PHI[[16]][k,5,])
  colnames(phi_df) = c('d=1', 'd=2', 'd=3', 'd=4', 'd=5')
  matplot(1:dim(phi_df)[1], phi_df, 
          type = 'l', ylab = 'phi', xlab = 'trials', 
          main = paste('V162209 Should be more tolerant of other moral standards: K=',k), ylim = c(0,1),
          col = c('black', 'green', 'blue', 'red', 'pink'), lty = 1:5, lwd = 1, cex.main = 0.75)
  legend('right', legend = colnames(phi_df), col = c('black', 'green', 'blue', 'red', 'pink'), lty = 1:5, lwd = 1)
}
```

```{r, fig.width=3, fig.height=2}
# Convergence checking for V162214
for (k in 1:K) {
  phi_df = cbind(PHI[[18]][k,1,], 
                 PHI[[18]][k,2,],
                 PHI[[18]][k,3,],
                 PHI[[18]][k,4,],
                 PHI[[18]][k,5,])
  colnames(phi_df) = c('d=1', 'd=2', 'd=3', 'd=4', 'd=5')
  matplot(1:dim(phi_df)[1], phi_df, 
          type = 'l', ylab = 'phi', xlab = 'trials', 
          main = paste('V162214 Blacks must try harder to get ahead: K=',k), ylim = c(0,1),
          col = c('black', 'green', 'blue', 'red', 'pink'), lty = 1:5, lwd = 1, cex.main = 0.75)
  legend('right', legend = colnames(phi_df), col = c('black', 'green', 'blue', 'red', 'pink'), lty = 1:5, lwd = 1)
}
```

```{r, fig.width=3, fig.height=2}
# Convergence checking for V162233
for (k in 1:K) {
  phi_df = cbind(PHI[[20]][k,1,], 
                 PHI[[20]][k,2,],
                 PHI[[20]][k,3,],
                 PHI[[20]][k,4,],
                 PHI[[20]][k,5,])
  colnames(phi_df) = c('d=1', 'd=2', 'd=3', 'd=4', 'd=5')
  matplot(1:dim(phi_df)[1], phi_df, 
          type = 'l', ylab = 'phi', xlab = 'trials', 
          main = paste('V162233 Do women complaining about discrimination cause more problem: K=',k), ylim = c(0,1),
          col = c('black', 'green', 'blue', 'red', 'pink'), lty = 1:5, lwd = 1, cex.main = 0.75)
  legend('right', legend = colnames(phi_df), col = c('black', 'green', 'blue', 'red', 'pink'), lty = 1:5, lwd = 1)
}
```

```{r, fig.width=3, fig.height=2}
# Convergence checking for V162269
for (k in 1:K) {
  phi_df = cbind(PHI[[24]][k,1,], 
                 PHI[[24]][k,2,],
                 PHI[[24]][k,3,],
                 PHI[[24]][k,4,],
                 PHI[[24]][k,5,])
  colnames(phi_df) = c('d=1', 'd=2', 'd=3', 'd=4', 'd=5')
  matplot(1:dim(phi_df)[1], phi_df, 
          type = 'l', ylab = 'phi', xlab = 'trials', 
          main = paste('V162269 US culture is generally harmed by immigrants: K=',k), ylim = c(0,1),
          col = c('black', 'green', 'blue', 'red', 'pink'), lty = 1:5, lwd = 1, cex.main = 0.75)
  legend('right', legend = colnames(phi_df), col = c('black', 'green', 'blue', 'red', 'pink'), lty = 1:5, lwd = 1)
}
```

```{r}
# Compare posterior predictive distribution with true marginal distribution and posterior samples
X_POST = X_SAMPLE

# sample from posterior predictive distribution
for (id in 1:dim(X_SAMPLE)[3]) {
  for (j in 1:p) {
    # get z_ij 
    z_cur = Z[,j,id]
    # get phi_ij
    prob = PHI[[j]][z_cur,,id] # number of entries x dj matrix
    prob_cum = prob%*%upper.tri(diag(ncol(prob)),diag=TRUE)
    Ran_unif <- runif(N)
    X_POST[,j,id] = rowSums(Ran_unif>prob_cum) + 1L
  }
}
```

```{r}
X_df = data.frame(X) #dataframe for original data without missing values
for (n_way in 1:2) {
  combinations = combn(1:p, n_way)

  # calculate true pmf without missingness
  true_pmf = c()
  for (i in 1:(dim(combinations)[2])) {
    variables = combinations[, i]
    true_pmf_temp = table(X_df[,variables])
    true_pmf_temp = c(true_pmf_temp/sum(true_pmf_temp))
    true_pmf = c(true_pmf, true_pmf_temp)
  }
  
  # calculate imputed pmf
  imputed_pmf = c()
  for (t in 1:(dim(X_POST)[3])) {
    # for each imputed dataset, calculate joint pmf
    X_POST_df = data.frame(X_POST[,,t])
    imputed_pmf_temp = c()
    for (i in 1:(dim(combinations)[2])) {
      variables = combinations[, i]
      imputed_pmf_temp = c(imputed_pmf_temp, c(table(X_POST_df[,variables])/sum(table(X_POST_df[,variables]))))
    }
    # imputed_pmf has dimension #samples x #pmfs
    imputed_pmf = rbind(imputed_pmf, imputed_pmf_temp)
  }
  
  # calculate mean and 95% credible interval
  imputed_pmf_mean = apply(imputed_pmf, MARGIN = 2, FUN = mean)
  imputed_pmf_ci = apply(imputed_pmf, MARGIN = 2, FUN = quantile, probs = c(0.025, 0.975))
  pmf_df <- data.frame(true_pmf,imputed_pmf_mean, imputed_pmf_ci[1,], imputed_pmf_ci[2,])
  names(pmf_df) <- c('true_pmf', "expectation","Q2.5", "Q97.5")
  
  # Calculate coverage
  lower_bound = imputed_pmf_ci[1,]
  upper_bound = imputed_pmf_ci[2,]
  coverage = (lower_bound<=true_pmf) & (true_pmf<=upper_bound)
  
  
  # Make a plot of true pmf vs posterior predictive pmf
  title = paste('True pmf vs Imputed pmf: n_way', n_way, 
                ', coverage', round(mean(coverage),2))
  plt <- ggplot(data = pmf_df, aes(x = true_pmf, y = expectation))+
    geom_pointrange(aes(ymin = Q2.5, ymax = Q97.5), size = 0.4, alpha = 0.6, colour = 'blue')+
    geom_abline(intercept = 0, slope = 1, colour = 'red', alpha = 0.2)+
    scale_x_continuous("true pmf", breaks = seq(0, 1, 0.05)) +
    scale_y_continuous(expression('imputed pmf')) +
    ggtitle(title)
  show(plt)
  
  plt <- ggplot(data = pmf_df, aes(x = true_pmf, y = expectation))+
    geom_point(size = 1, alpha = 0.7, colour = 'blue')+
    geom_abline(intercept = 0, slope = 1, colour = 'red', alpha = 0.5)+
    scale_x_continuous("true pmf", breaks = seq(0, 1, 0.05)) +
    scale_y_continuous(expression('imputed pmf')) +
    ggtitle(title)
  show(plt)
  print(dim(pmf_df))
  
  
  # make coverage plot
  title = paste('95% CI: n_way', n_way, 
                ', coverage', round(mean(coverage),2))
  PredCI_col <- ifelse(coverage==1,"lightblue3","red4")
  PredCI_lwd <- ifelse(coverage==1,1,2)
  plot(pmf_df[,1], pch=4, ylim=range(pretty(c(pmf_df[,3], pmf_df[,4]))),
       xlab="Index", ylab="Joint probabilities", las=1,col="orange4",
       main=title)
  segments(seq_len(length(pmf_df[,1])), pmf_df[,3], y1=pmf_df[,4], lend=1,
           lwd=PredCI_lwd,col=PredCI_col)
  points(pmf_df[,3], pch="-", bg=PredCI_col); points(pmf_df[,4], pch="-", bg=PredCI_col)
  legend("topright", inset=.05, bty="n",
         legend=c("Intervals containing true pmf",
                  "Intervals NOT containing true pmf","true joint probabilities"),
         lwd=c(1,2,1), lty=c(1,1,NA),pch=c(NA,NA,4), col=c("lightblue3","red4","orange4"))
}
```


```{r}
# save data: X_POST here is actually sampled from posterior predictive distribution
save(BETA, PHI, X, X_POST, Z, file = "./samples_Sort_Beta/post_samples_sort_beta.RData")
```

* * *



