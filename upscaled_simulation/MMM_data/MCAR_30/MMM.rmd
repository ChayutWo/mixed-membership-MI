
---
title: "Mixed Membership Model for MI - Simulation example with 30% MCAR"
output:
  pdf_document: default
  html_document:
    highlight: pygments
    theme: spacelab
---

```{r setup, echo =FALSE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.pos = 'h', fig.align = 'center')
knitr::opts_chunk$set(fig.cap = "",  fig.path = "Plot")
library(knitr)
library(dplyr)
library(arm)
library(pROC)
library(tidyverse)
library(MASS)
library(tigerstats)
library(leaps)
library(car)
library(rms)
require(caret)
require(e1071)
library(lme4) 
library(lattice)
library(broom)
library(boot)
library(ggplot2)
library(cobalt)
require(tidyverse)
require(rstanarm)
require(magrittr)
require(rstan)
require(MCMCpack)
library(abind)
library(matrixStats)
library(truncnorm)
library(mvtnorm)
library(MCMCpack)
library(mnormt)
library(coda)
```

* * *
### Generate small dataset

Settings 
- N = 2500 surveyed subjects
- K = 5 clusters
- p = 5 questions/variables
- d1 = 2 choices, d2 = d3 = d4 = 3 choices, d5 = 5 choices
- $\gamma$ = 2 and $\alpha_0$ = 1 for DP generative process

Generative process
- $\beta|\gamma \sim GEM(\gamma)$
- $\pi_i|\alpha_0 \sim DP(\alpha_0, \beta)$ for i = 1,...,N
- $z_{ij}|\pi_i \sim Cat(\pi_i)$ for i = 1,...,N and j = 1,...,p
- $x_{ij}|z_{ij}=k \sim Cat(\Phi^{(j)}_k)$ for i = 1,...,N and j=1,...,p

```{r}
set.seed(1)
# parameter setup
N = 2500 # number of surveyed subjects
K_true = 5 # number of true clusteres
p = 5 # number of survey questions
d1 = 2 # number of levels of 1st variable
d2 = 3 # number of levels of 2st variable
d3 = 3 # number of levels of 3st variable
d4 = 3
d5 = 5
gamma_true = 2
alpha_true = 1
level = c(d1, d2, d3, d4, d5)

# place holder
Phi_1 = matrix(NA, nrow = K_true, ncol = d1) # multinomial parameter for 1st variable
Phi_2 = matrix(NA, nrow = K_true, ncol = d2) # multinomial parameter for 2nd variable
Phi_3 = matrix(NA, nrow = K_true, ncol = d3) # multinomial parameter for 3rd variable
Phi_4 = matrix(NA, nrow = K_true, ncol = d4) # multinomial parameter for 4th variable
Phi_5 = matrix(NA, nrow = K_true, ncol = d5) # multinomial parameter for 5th variable
```

```{r}
# Phi parameter for different clusters and different variables
# size #clusterx#levels

# 1st variable (d1 = 2)
Phi_1[,1] = c(0.1, 0.3, 0.5, 0.7, 0.9)
Phi_1[,2] = 1 - Phi_1[,1]

# 2nd variable (d2 = 3)
Phi_2[,1] = c(0.1, 0.1, 0.8, 0.25, 0.5)
Phi_2[,2] = c(0.1, 0.8, 0.1, 0.5, 0.25)
Phi_2[,3] = 1 - Phi_2[,1] - Phi_2[,2]

# 3rd variable (d3 = 3)
Phi_3[,1] = c(0.1, 0.8, 0.1, 0.25, 0.25)
Phi_3[,2] = c(0.8, 0.1, 0.1, 0.25, 0.5)
Phi_3[,3] = 1 - Phi_3[,1] - Phi_3[,2]

# 4th variable (d4 = 3)
Phi_4[,1] = c(0.8, 0.1, 0.1, 0.5, 0.25)
Phi_4[,2] = c(0.1, 0.1, 0.8, 0.25, 0.5)
Phi_4[,3] = 1 - Phi_4[,1] - Phi_4[,2]

# 5th variable (d5= 5)
Phi_5[,1] = c(0.5, 0.1, 0.1, 0.1, 0.2)
Phi_5[,2] = c(0.2, 0.5, 0.1, 0.1, 0.1)
Phi_5[,3] = c(0.1, 0.2, 0.5, 0.1, 0.1)
Phi_5[,4] = c(0.1, 0.1, 0.2, 0.5, 0.1)
Phi_5[,5] = 1 - Phi_5[,1] - Phi_5[,2] - Phi_5[,3] - Phi_5[,4]

Phi = list(Phi_1, Phi_2, Phi_3, Phi_4, Phi_5)
```

```{r}
# Beta ~ GEM(gamma_true)
Beta_true = array(NA, dim = K_true)
V = array(NA, K_true)
V[K_true] = 1
prod = 1 # product of (1-V_l) for l<k
# Sample V_k from k = 1,...,K-1
for (k in 1:(K_true - 1)) {
  V[k] = rbeta(1, 1, gamma_true) # Vk ~ Beta(1, gamma)
  Beta_true[k] = V[k]*prod
  prod = prod*(1-V[k])
}
Beta_true[K_true] = 1-sum(Beta_true[1:(K_true-1)])
```

```{r}
# Pi_i ~ DP(alpha_true, Beta_true) for i = 1,...,N
Pi_true = matrix(NA, N, K_true)
u = matrix(NA, N, K_true)
u[,K_true] = 1
prod = array(1, N) # product of (1-u_il) for l<k
# sample u_ik ~ Beta distribution
# then, obtain Pi from u
for (k in 1:(K_true - 1)) {
  u[,k] = rbeta(N, alpha_true*Beta_true[k], alpha_true*(1-sum(Beta_true[1:k])))
  Pi_true[,k] = u[,k]*prod
  prod = prod*(1-u[,k])
}
Pi_true[,K_true] = 1-rowSums(Pi_true[,1:(K_true-1)])
```

```{r}
# Check the mean of Pi from DP
cat('Beta:', Beta_true,'\n')
cat('Pi:', apply(Pi_true, MARGIN = 2, FUN = mean))
```

```{r}
# Sample cluster assignment z_ij ~ Cat(Pi_i)
# Sample observation x_ij|z_ij=k ~ Cat(Phi_jk)
z_true = matrix(NA, nrow = N, ncol = p) # true cluster assignment
X = matrix(NA, nrow = N, ncol = p) # data matrix
for (i in 1:N) {
  z_true[i,] = sample(1:K_true, p, replace=TRUE, prob=Pi_true[i,])
  for (j in 1:p) {
    z = z_true[i,j]
    prob = Phi[[j]][z,]
    X[i,j] = sample(1:length(prob), 1, replace = TRUE, prob = prob)
  }
}

```

```{r}
# make MCAR
missing_prob = 0.3
X_miss = X
for (col in 1:ncol(X)) {
    missing_ind <- rbernoulli(N, p = missing_prob)
    X_miss[missing_ind, col] <- NA
}
R = is.na(X_miss)
# initial imputation
for (col in 1:ncol(X)) {
    missing_ind <- R[,col]
    X_miss[missing_ind, col] <- sample(1:level[col], size = sum(missing_ind),
                                       table(X_miss[R[,col]!=1,col]), replace = TRUE)
}
```

### Visualization

```{r}
barplot(table(z_true)/N/p, main = 'cluster assignment: z', ylim = c(0,1))
```

```{r}
# 1st variable
var = 1
barplot(table(X[,var])/N, main = 'overall')
for (z in 1:K_true) {
  barplot(table(X[z_true[,var]==z,var])/sum(table(X[z_true[,var]==z,var])), 
        main = paste('variable:',var,'/', 'cluster:',z), ylim = c(0,1))
}
```

```{r}
# 2nd variable
var = 2
barplot(table(X[,var])/N, main = 'overall')
for (z in 1:K_true) {
  barplot(table(X[z_true[,var]==z,var])/sum(table(X[z_true[,var]==z,var])), 
        main = paste('variable:',var,'/', 'cluster:',z), ylim = c(0,1))
}
```

```{r}
# 3rd variable
var = 3
barplot(table(X[,var])/N, main = 'overall')
for (z in 1:K_true) {
  barplot(table(X[z_true[,var]==z,var])/sum(table(X[z_true[,var]==z,var])), 
        main = paste('variable:',var,'/', 'cluster:',z), ylim = c(0,1))
}
```

```{r}
# 4th variable
var = 4
barplot(table(X[,var])/N, main = 'overall')
for (z in 1:K_true) {
  barplot(table(X[z_true[,var]==z,var])/sum(table(X[z_true[,var]==z,var])), 
        main = paste('variable:',var,'/', 'cluster:',z), ylim = c(0,1))
}
```

```{r}
# 5th variable
var = 5
barplot(table(X[,var])/N, main = 'overall')
for (z in 1:K_true) {
  barplot(table(X[z_true[,var]==z,var])/sum(table(X[z_true[,var]==z,var])), 
        main = paste('variable:',var,'/', 'cluster:',z), ylim = c(0,1))
}
```

### Mixed-Membership Model (MMM) MCMC

```{r}
set.seed(1)
# MCMC variables
Mon = 20000
burnin = 10000
thining = 50

N = dim(X)[1]
K = 60
p = dim(X)[2]
# hyperprior parameters
# gamma ~ Gamma(a,b)
a = 0.25
b = 0.25
# alpha0 ~ Gamma(c,d)
c = 0.25
d = 0.25
# storage
GAMMA = rep(NA, (Mon-burnin)/thining)
ALPHA0 = rep(NA, (Mon-burnin)/thining)
BETA = matrix(NA, nrow = (Mon-burnin)/thining, ncol = K)
PI = array(NA, c(N, K, (Mon-burnin)/thining))
Z = array(NA, c(N, p, (Mon-burnin)/thining))
X_SAMPLE = array(NA, c(N, p, (Mon-burnin)/thining))
PHI1 = array(NA, c(K, d1, (Mon-burnin)/thining))
PHI2 = array(NA, c(K, d2, (Mon-burnin)/thining))
PHI3 = array(NA, c(K, d3, (Mon-burnin)/thining))
PHI4 = array(NA, c(K, d4, (Mon-burnin)/thining))
PHI5 = array(NA, c(K, d5, (Mon-burnin)/thining))
```

```{r}
# initialization for x
x_temp = X_miss

# initialization for z
z_temp = matrix(sample(1:K, size = N*p, replace = TRUE), nrow = N, ncol = p)
# nik = # of variables in cluster k for observation i
nik = matrix(NA, N, K)
for (k in 1:K) {
  nik[,k] = rowSums(z_temp==k)
}
# initialization for PHI
phi_temp = list()
for (j in 1:p) {
  prob = rdirichlet(n = K, rep(1,level[j])) # Kxlevel matrix
  phi_temp[[j]] = prob
}
# initialization for gamma and alpha0
gamma_temp = 1
alpha0_temp = 1 

# initialization for Vk and Betak
# Beta ~ GEM(gamma_temp)
# Sample Vk ~ Beta(1, gamma)
Vk_temp <- matrix(rbeta(K-1,1,gamma_temp),nrow=K-1)
Vk_temp <- rbind(Vk_temp,1)
one_min_Vk_temp <- 1L-Vk_temp
one_min_Vk_temp <- c(1,cumprod(one_min_Vk_temp[1:(K-1)]))
# Compute Betak for k = 1,...,K
beta_temp <- Vk_temp*one_min_Vk_temp

# initialization for u and Pi
# Pi_i ~ DP(alpha_temp, Beta_temp) for i = 1,...,N
one_min_cumsum_beta_temp <-1L-cumsum(beta_temp[1:(K-1)])
one_min_cumsum_beta_temp[one_min_cumsum_beta_temp<=0] <- 1.0e-20
u_temp <- matrix(rbeta(N*(K-1),alpha0_temp*rep(beta_temp[1:(K-1)],each=N),
                       alpha0_temp*rep(one_min_cumsum_beta_temp,each=N) ),
                 nrow=N,ncol=(K-1),byrow = F)
# Prevent instability issue
u_temp[u_temp==1] = 0.99999; u_temp[u_temp==0] = 1.0e-5
u_temp <- cbind(u_temp,1)
one_min_u_temp <- 1L-u_temp
one_min_u_temp <- cbind(1,t(apply(one_min_u_temp[,-K],1,cumprod)))
# Compute Pi from uik
Pi_temp <- u_temp*one_min_u_temp

# initialization for auxiliary variables
t_temp = array(NA, N)
sik = matrix(0, N, K)
```

```{r}
id = 0
for (trial in 1:Mon) {
  # Step 1: Update z_ij
  for (j in 1:p) {
    # get phi_j
    prob = phi_temp[[j]]
    var = x_temp[,j]
    likelihood = t(prob[,var]) # NxK of phi_j_xij
    full_prob_matrix = Pi_temp*likelihood
    full_prob_matrix <- full_prob_matrix/matrix(rowSums(full_prob_matrix),nrow=N,ncol=K) # normalize
    Ran_unif <- runif(N)
    cumul <- full_prob_matrix%*%upper.tri(diag(ncol(full_prob_matrix)),diag=TRUE)
    z_temp[,j] <- rowSums(Ran_unif>cumul) + 1L
  }
  # update nik
  nik = matrix(NA, N, K)
  for (k in 1:K) {
    nik[,k] = rowSums(z_temp==k)
  }
  # Step 2: Update Phi_k
  for (j in 1:p) {
    for (k in 1:K) {
      dj = level[j]
      njk = rep(NA, dj) # number of response j in cluster k that answer different levels
      for (lev in 1:dj) {
        # number of response j that has level lev and under cluster k
        njk[lev] = sum(x_temp[,j]==lev & z_temp[,j]==k)
      }
      phi_temp[[j]][k,] = rdirichlet(1,rep(1,level[j])+njk)
    }
  }
  # Step 3: Update Vk and betak
  # sample auxiliary variable ti
  t_temp = array(NA, N)
  sik = matrix(0, N, K)
  for (i in 1:N) {
    t_temp[i] = rbeta(1,alpha0_temp,sum(nik[i,]))
    # sample auxiliary variable sik
    for (k in 1:K) {
      if (nik[i,k]==0) {
        sik[i,k]=0
      }else{
        prob = (alpha0_temp*beta_temp[k])/(alpha0_temp*beta_temp[k]+1:(nik[i,k])-1)
        sik[i,k] = sum(rbernoulli(nik[i,k], prob))
      }
    }
  }
  
  # sample Vk for k = 1,...,K-1
  Vk_temp[1:(K-1)] <- rbeta((K-1),(1L+colSums(sik[,1:(K-1)])),
                            (gamma_temp + (sum(sik) - cumsum(colSums(sik)))[1:K-1]) )
  # Prevent instability issue
  Vk_temp[Vk_temp==1] <- 0.99999; Vk_temp[Vk_temp==0] <- 1.0e-5
  Vk_temp[K] <- 1
  one_min_Vk_temp <- 1L-Vk_temp
  one_min_Vk_temp <- c(1,cumprod(one_min_Vk_temp[1:(K-1)]))
  # compute betak for k = 1,...,K
  beta_temp <- Vk_temp*one_min_Vk_temp
  if (sum(beta_temp<0)>0) {
    print('error with beta < 0')
  }
  
  # Step 4: Update u_ik and pi_ik
  one_min_cumsum_beta_temp <- 1L-cumsum(beta_temp[1:(K-1)])
  # Prevent instability issue
  one_min_cumsum_beta_temp[one_min_cumsum_beta_temp<=0] <- 1.0e-20
  u_temp[,1:(K-1)] <- matrix(rbeta( (N*(K-1)), (alpha0_temp*rep(beta_temp[1:(K-1)],each=N) + c(nik[,1:(K-1)]) ) ,
                         (alpha0_temp*rep(one_min_cumsum_beta_temp,each=N) +
                            c(matrix(rowSums(nik),ncol=K-1,nrow=N)-t(apply(nik,1,cumsum))[,1:K-1]) ) ),
                         nrow=N,ncol=(K-1),byrow = F)
  # Prevent instability issue
  u_temp[u_temp==1] = 0.99999; u_temp[u_temp==0] = 1.0e-5
  u_temp[,K] <- 1
  one_min_u_temp <- 1L-u_temp
  one_min_u_temp <- cbind(1,t(apply(one_min_u_temp[,-K],1,cumprod)))
  Pi_temp <- u_temp*one_min_u_temp
  
  # Step 5.1: Update gamma
  gamma_temp = rgamma(1, a + K - 1, b - sum(log(1-Vk_temp[1:(K-1)])))
  # Step 5.2: Update alpha
  alpha0_temp = rgamma(1, c + sum(sik), d - sum(log(t_temp)))
  
  # Step 6: Update x_ij
  for (i in 1:N) {
    for (j in 1:p) {
      if (R[i,j] == 1) {
        # if the value is missing, sample from posterior discrete distribution
        z_cur = z_temp[i,j]
        prob = phi_temp[[j]][z_cur,]
        prob_cum = prob%*%upper.tri(diag(length(prob)),diag=TRUE)
        Ran_unif <- runif(1)
        x_temp[i,j] <- sum(Ran_unif>prob_cum) + 1L
      }
    }
  }
  # Save posterior samples
  if ((trial>burnin) && ((trial-burnin)%%thining==1) ) {
    id = id + 1
    GAMMA[id] = gamma_temp
    ALPHA0[id] = alpha0_temp
    BETA[id,] = beta_temp
    PI[,,id] = Pi_temp
    Z[,,id] = z_temp
    X_SAMPLE[,,id] = x_temp
    PHI1[,,id] = phi_temp[[1]]
    PHI2[,,id] = phi_temp[[2]]
    PHI3[,,id] = phi_temp[[3]]
    PHI4[,,id] = phi_temp[[4]]
    PHI5[,,id] = phi_temp[[5]]
  }
  print(paste('finish runing MCMC trial:',trial))
}
```

### Posterior Diagnostics

```{r, echo = FALSE}
gamma.mcmc <- mcmc(GAMMA, start = 1)
summary(gamma.mcmc)
plot(gamma.mcmc)
autocorr.plot(gamma.mcmc)
```

```{r}
alpha0.mcmc <- mcmc(ALPHA0, start = 1)
summary(alpha0.mcmc)
plot(alpha0.mcmc)
autocorr.plot(alpha0.mcmc)
```

```{r}
# Convergence checking Beta a after accounting for burnin and thining
beta_df = BETA
colnames(beta_df) <- 1:K
matplot(1:dim(beta_df)[1], beta_df, 
        type = 'l', ylab = 'Beta_k', xlab = 'trials', 
        main = 'Checking stability of sampled Beta')

```

```{r}
# Check number of active cluster 
Z_df = Z
active_cluster = array(0, dim(Z_df)[3])
for (idx in 1:dim(active_cluster)) {
  for (k in 1:K) {
    if (sum(Z_df[,,idx]==k)!=0) {
      active_cluster[idx] = active_cluster[idx] + 1
    }
  }
}
plot(1:dim(active_cluster), active_cluster, type = 'l', ylab = 'Number of active clusters',
     xlab = 'trials', main = 'Checking number of active clusters during MCMC', ylim = c(1,K))

```


```{r, fig.width=3, fig.height=2}
# Convergence checking for Phi1
for (k in 1:K) {
  phi_df = cbind(PHI1[k,1,], 
                 PHI1[k,2,])
  colnames(phi_df) = c('d=1', 'd=2')
  matplot(1:dim(phi_df)[1], phi_df, 
          type = 'l', ylab = 'phi', xlab = 'trials', 
          main = paste('Checking stability of sampled phi1: K=',k), ylim = c(0,1),
          col = c('black', 'green'), lty = 1:2, lwd = 1)
  legend('right', legend = colnames(phi_df), col = c('black', 'green'), lty = 1:2, lwd = 1)
}
```

```{r}
# Convergence checking for Phi2
for (k in 1:K) {
  phi_df = cbind(PHI2[k,1,], 
                 PHI2[k,2,],
                 PHI2[k,3,])
  colnames(phi_df) = c('d=1', 'd=2', 'd=3')
  matplot(1:dim(phi_df)[1], phi_df, 
          type = 'l', ylab = 'phi', xlab = 'trials', 
          main = paste('Checking stability of sampled phi2: K=',k), ylim = c(0,1), 
          col = c('black', 'green', 'red'), lty = 1:3, lwd = 1)
  legend('right', legend = colnames(phi_df), col = c('black', 'green', 'red'), lty = 1:3, lwd = 1)
}
```

```{r}
# Convergence checking for Phi3
for (k in 1:K) {
  phi_df = cbind(PHI3[k,1,], 
                 PHI3[k,2,],
                 PHI3[k,3,])
  colnames(phi_df) = c('d=1', 'd=2', 'd=3')
  matplot(1:dim(phi_df)[1], phi_df, 
          type = 'l', ylab = 'phi', xlab = 'trials', 
          main = paste('Checking stability of sampled phi3: K=',k), ylim = c(0,1), 
          col = c('black', 'green', 'red'), lty = 1:3, lwd = 1)
  legend('right', legend = colnames(phi_df), col = c('black', 'green', 'red'), lty = 1:3, lwd = 1)
}
```

```{r}
# Convergence checking for Phi4
for (k in 1:K) {
  phi_df = cbind(PHI4[k,1,], 
                 PHI4[k,2,],
                 PHI4[k,3,])
  colnames(phi_df) = c('d=1', 'd=2', 'd=3')
  matplot(1:dim(phi_df)[1], phi_df, 
          type = 'l', ylab = 'phi', xlab = 'trials', 
          main = paste('Checking stability of sampled phi4: K=',k), ylim = c(0,1), 
          col = c('black', 'green', 'red'), lty = 1:3, lwd = 1)
  legend('right', legend = colnames(phi_df), col = c('black', 'green', 'red'), lty = 1:3, lwd = 1)
}
```

```{r}
# Convergence checking for Phi5
for (k in 1:K) {
  phi_df = cbind(PHI5[k,1,], 
                 PHI5[k,2,],
                 PHI5[k,3,],
                 PHI5[k,4,],
                 PHI5[k,5,])
  colnames(phi_df) = c('d=1', 'd=2', 'd=3', 'd=4', 'd=5')
  matplot(1:dim(phi_df)[1], phi_df, 
          type = 'l', ylab = 'phi', xlab = 'trials', 
          main = paste('Checking stability of sampled phi5: K=',k), ylim = c(0,1), 
          col = c('black', 'green', 'red', 'blue', 'yellow'), lty = 1:5, lwd = 1)
  legend('right', legend = colnames(phi_df), col = c('black', 'green', 'red', 'blue', 'yellow'), 
         lty = 1:5, lwd = 1)
}
```

```{r}
# Compare predictive distribution with true marginal distribution and posterior samples
X_POST = X_SAMPLE
for (var in 1:p) {
  true_pmf = table(X[,var])/N
  imputed_pmf = table(X_POST[,var,])
  imputed_pmf = imputed_pmf/sum(imputed_pmf)
  df = rbind(imputed_pmf, true_pmf)
  barplot(df, xlab = 'Category', beside = TRUE, 
        legend = TRUE, main = paste('Posterior Predictive Distribution Assessment: j =', var), 
        ylim = c(0,1))
}
```

```{r}
# Evaluate bivariate pmf
n_way = 2
combinations = combn(1:p, n_way)
for (i in 1:(dim(combinations)[2])) {
  variables = combinations[, i]
  var1 = variables[1]
  var2 = variables[2]
  # calculate true pmf without missingness
  true_pmf = table(X[,var1], X[,var2])
  true_pmf = c(true_pmf/sum(true_pmf))
  # calculate imputed pmf
  imputed_pmf = table(X_POST[,var1,], X_POST[,var2,])
  imputed_pmf = c(imputed_pmf/sum(imputed_pmf))
  df = rbind(imputed_pmf, true_pmf)
  # barplot comparing true pmf and imputed pmf
  barplot(df, xlab = 'Category', beside = TRUE, 
      legend = TRUE, 
      main = paste('Posterior Predictive Distribution Assessment: joint pmf variable', var1, 'and', var2),
      ylim = c(0,0.6))
}
print(df)
```
* * *



